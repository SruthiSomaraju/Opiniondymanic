{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1088361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "# import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "from utils.utils import loadWord2Vec, clean_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bdb6637",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'R8'\n",
    "\n",
    "word_embeddings_dim = 300\n",
    "word_vector_map = {}\n",
    "\n",
    "# shulffing\n",
    "doc_name_list = []\n",
    "doc_train_list = []\n",
    "doc_test_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1215cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cleaned_data/' + dataset + '/' + dataset + '.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc_name_list.append(line.strip())\n",
    "        temp = line.split(\"\\t\")\n",
    "        if temp[1].find('test') != -1:\n",
    "            doc_test_list.append(line.strip())\n",
    "        elif temp[1].find('train') != -1:\n",
    "            doc_train_list.append(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f52dfccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content_list = []\n",
    "with open('./cleaned_data/' + dataset + '/' + dataset + '_clean.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc_content_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fc1c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = []\n",
    "for train_name in doc_train_list:\n",
    "    train_id = doc_name_list.index(train_name)\n",
    "    train_ids.append(train_id)\n",
    "# print(train_ids)\n",
    "random.shuffle(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "236f9b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
    "with open('./cleaned_data/' + dataset + '/graph/' + dataset + '.train.index', 'w') as f:\n",
    "    f.write(train_ids_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cca6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = []\n",
    "for test_name in doc_test_list:\n",
    "    test_id = doc_name_list.index(test_name)\n",
    "    test_ids.append(test_id)\n",
    "\n",
    "# print(test_ids)\n",
    "random.shuffle(test_ids)\n",
    "\n",
    "test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
    "with open('./cleaned_data/' + dataset + '/graph/' + dataset + '.test.index', 'w') as f:\n",
    "    f.write(test_ids_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "760aed56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size 7674\n",
      "adding...\n"
     ]
    }
   ],
   "source": [
    "ids = train_ids + test_ids\n",
    "# print(ids)\n",
    "print(\"data size {}\".format(len(ids)))\n",
    "\n",
    "print(\"adding...\")\n",
    "shuffle_doc_name_list = []\n",
    "shuffle_doc_words_list = []\n",
    "for id in ids:\n",
    "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
    "shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb329892",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cleaned_data/' + dataset + '/' + dataset + '_shuffle.txt', 'w') as f:\n",
    "    f.write(shuffle_doc_name_str)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/corpus/' + dataset + '_shuffle.txt', 'w') as f:\n",
    "    f.write(shuffle_doc_words_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91e34652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building...\n"
     ]
    }
   ],
   "source": [
    "# build vocab\n",
    "\n",
    "print(\"building...\")\n",
    "word_freq = {}\n",
    "word_set = set()\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c189128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc_list = {}\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in word_doc_list:\n",
    "            doc_list = word_doc_list[word]\n",
    "            doc_list.append(i)\n",
    "            word_doc_list[word] = doc_list\n",
    "        else:\n",
    "            word_doc_list[word] = [i]\n",
    "        appeared.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50631fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tonnes', 'compared', 'determined', 'c', 'shipments', 'selling', 'co', 'ship', 'annual', 'made', 'inoc', 'delivery', 'wants', 'said', 'eight', 'needs', 'deliver', 'suez', 'light', 'oil', 'last', 'contract', 'plans', 'approval', 'petroleum', 'begin', 'itoh', 'supply', 'gulf', 'later', 'recommendation', 'four', 'considered', 'lift', 'government', 'mln', 'may', 'corp', 'spot', 'one', 'iraq', 'finalised', 'planned', 'sri', 'indian', 'expired', 'national', 'cabinet', 'crude', 'december', 'reuter', 'imports', 'told', 'three', 'port', 'market', 'two', 'price', 'egyptian', 'june', 'buy', 'lifting', 'small', 'renew', 'requirements', 'several', 'since', 'reuters', 'take', 'lanka', 'cpc', 'tonne', 'upper', 'accept', 'sea', 'received', 'year', 'company', 'officials', 'red', 'yet', 'ships', 'also', 'bought', 'could', 'abu', 'place', 'reply', 'dhabi', 'awaiting', 'decided', 'constraints', 'course', 'agreed', 'going', 'renewed', 'proposed'}\n"
     ]
    }
   ],
   "source": [
    "print(appeared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "594e0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "word_id_map = {}\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i\n",
    "\n",
    "vocab_str = '\\n'.join(vocab)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/corpus/' + dataset + '_vocab.txt', 'w') as f:\n",
    "    f.write(vocab_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d57b3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label list\n",
    "label_set = set()\n",
    "for doc_meta in shuffle_doc_name_list:\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label_set.add(temp[2])\n",
    "label_list = list(label_set)\n",
    "\n",
    "label_list_str = '\\n'.join(label_list)\n",
    "with open('./cleaned_data/' + dataset + '/corpus/' + dataset + '_labels.txt', 'w') as f:\n",
    "    f.write(label_list_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85a9c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x: feature vectors of training docs, no initial features\n",
    "# slect 90% training set\n",
    "train_size = len(train_ids)\n",
    "val_size = int(0.1 * train_size)\n",
    "real_train_size = train_size - val_size  # - int(0.5 * train_size)\n",
    "\n",
    "\n",
    "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
    "real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/' + dataset + '.real_train.name', 'w') as f:\n",
    "    f.write(real_train_doc_names_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5edabb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_x = []\n",
    "col_x = []\n",
    "data_x = []\n",
    "for i in range(real_train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            # print(doc_vec)\n",
    "            # print(np.array(word_vector))\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_x.append(i)\n",
    "        col_x.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_x.append(doc_vec[j] / doc_len)  # doc_vec[j]/ doc_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "460f8919",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(real_train_size, word_embeddings_dim))\n",
    "y = []\n",
    "for i in range(real_train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    y.append(one_hot)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "214ce46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tx: feature vectors of test docs, no initial features\n",
    "test_size = len(test_ids)\n",
    "\n",
    "row_tx = []\n",
    "col_tx = []\n",
    "data_tx = []\n",
    "for i in range(test_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i + train_size]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_tx.append(i)\n",
    "        col_tx.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_tx.append(doc_vec[j] / doc_len)  # doc_vec[j] / doc_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13002d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),shape=(test_size, word_embeddings_dim))\n",
    "\n",
    "ty = []\n",
    "for i in range(test_size):\n",
    "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ty.append(one_hot)\n",
    "ty = np.array(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f2a0379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)\n"
     ]
    }
   ],
   "source": [
    "# allx: the the feature vectors of both labeled and unlabeled training instances\n",
    "# (a superset of x)\n",
    "# unlabeled training instances -> words\n",
    "\n",
    "word_vectors = np.random.uniform(-0.01, 0.01,(vocab_size, word_embeddings_dim))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    if word in word_vector_map:\n",
    "        vector = word_vector_map[word]\n",
    "        word_vectors[i] = vector\n",
    "\n",
    "row_allx = []\n",
    "col_allx = []\n",
    "data_allx = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i))\n",
    "        col_allx.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_allx.append(doc_vec[j] / doc_len)  # doc_vec[j]/doc_len\n",
    "for i in range(vocab_size):\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i + train_size))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(word_vectors.item((i, j)))\n",
    "\n",
    "\n",
    "row_allx = np.array(row_allx)\n",
    "col_allx = np.array(col_allx)\n",
    "data_allx = np.array(data_allx)\n",
    "\n",
    "allx = sp.csr_matrix((data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n",
    "\n",
    "ally = []\n",
    "for i in range(train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ally.append(one_hot)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    ally.append(one_hot)\n",
    "\n",
    "ally = np.array(ally)\n",
    "\n",
    "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef1f9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Doc word heterogeneous graph\n",
    "'''\n",
    "\n",
    "# word co-occurence with context windows\n",
    "window_size = 20\n",
    "windows = []\n",
    "\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        # print(length, length - window_size + 1)\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "            #print(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7be813a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fcfce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pair_count = {}\n",
    "for window in windows:\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = word_id_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = word_id_map[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "# pmi as weights\n",
    "\n",
    "num_window = len(windows)\n",
    "\n",
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "    pmi = log((1.0 * count / num_window) / (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size + i)\n",
    "    col.append(train_size + j)\n",
    "    weight.append(pmi)\n",
    "\n",
    "\n",
    "# doc word frequency\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[doc_id]\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
    "                  word_doc_freq[vocab[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)\n",
    "\n",
    "node_size = train_size + vocab_size + test_size\n",
    "adj = sp.csr_matrix(\n",
    "    (weight, (row, col)), shape=(node_size, node_size))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump objects\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.x', 'wb') as f:\n",
    "    pkl.dump(x, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.y', 'wb') as f:\n",
    "    pkl.dump(y, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.tx', 'wb') as f:\n",
    "    pkl.dump(tx, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.ty', 'wb') as f:\n",
    "    pkl.dump(ty, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.allx', 'wb') as f:\n",
    "    pkl.dump(allx, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.ally', 'wb') as f:\n",
    "    pkl.dump(ally, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.adj', 'wb') as f:\n",
    "    pkl.dump(adj, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
