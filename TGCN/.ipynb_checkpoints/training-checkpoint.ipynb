{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e8dd5ab",
   "metadata": {
    "id": "3e8dd5ab"
   },
   "outputs": [],
   "source": [
    "# Initializing all required variables\n",
    "dataset = 'data'\n",
    "lr = 0.02  \n",
    "epochs  = 500  \n",
    "hidden1 = 200  \n",
    "dropout = 0.5  \n",
    "weight_decay = 0.   \n",
    "early_stopping = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b386dce3",
   "metadata": {
    "id": "b386dce3"
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77f00984",
   "metadata": {
    "id": "77f00984"
   },
   "outputs": [],
   "source": [
    "# user-defined functions\n",
    "s = 6606\n",
    "np.random.seed(s)\n",
    "tf.random.set_seed(s)\n",
    "\n",
    "#softmax\n",
    "def masked_softmax_cross_entropy(predictions, labels, mask_): \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=labels)\n",
    "    \n",
    "    mask_ = tf.cast(mask_, dtype=tf.float32)\n",
    "    mask_ /= tf.reduce_mean(mask_)\n",
    "    loss *= mask_\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "#accuracy\n",
    "def masked_accuracy(predictions, labels, mask_):\n",
    "    \"\"\"\n",
    "    Accuracy with masking.\n",
    "    \"\"\"\n",
    "    correct_predictions = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "    acc_all = tf.cast(correct_predictions, tf.float32)\n",
    "    mask_= tf.cast(mask_, dtype=tf.float32)\n",
    "    mask_ /= tf.reduce_mean(mask_)\n",
    "    acc_all *= mask_\n",
    "    return tf.reduce_mean(acc_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c498b30",
   "metadata": {
    "id": "2c498b30"
   },
   "outputs": [],
   "source": [
    "# Some other user-defined functions\n",
    "def parse_index_file(file_name):\n",
    "    ind = []\n",
    "    for l in open(file_name):\n",
    "        ind.append(int(l.strip()))\n",
    "    return ind\n",
    "\n",
    "#for sample masking\n",
    "def sample_mask(index, l):\n",
    "    mask_ = np.zeros(l)\n",
    "    mask_[index] = 1\n",
    "    return np.array(mask_, dtype=np.bool)\n",
    "\n",
    "#to load corpus\n",
    "def load_corpus(dataset_str):\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
    "    objs = []\n",
    "    for i in range(len(names)):\n",
    "        with open('./cleaned_data2/' + dataset_str + '/graph/ind.' + dataset_str + '.' + names[i], 'rb') as fp:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objs.append(pkl.load(fp, encoding='latin1'))\n",
    "            else:\n",
    "                objs.append(pkl.load(fp))\n",
    "\n",
    "    X, y, tx, ty, all_x, all_y, adj = tuple(objs)\n",
    "\n",
    "    features = sp.vstack((all_x, tx)).tolil()\n",
    "    labels = np.vstack((all_y, ty))\n",
    "   \n",
    "    train_indx_orig = parse_index_file('./cleaned_data2/' + dataset_str + '/graph/' + dataset_str + '.train.index')\n",
    "    train_size = len(train_indx_orig)\n",
    "\n",
    "    valid_size = train_size - X.shape[0]\n",
    "    test_size = tx.shape[0]\n",
    "\n",
    "    indx_train = range(len(y))\n",
    "    indx_val = range(len(y), len(y) + valid_size)\n",
    "    indx_test = range(all_x.shape[0], all_x.shape[0] + test_size)\n",
    "\n",
    "    train_mask_ = sample_mask(indx_train, labels.shape[0])\n",
    "    valid_mask_ = sample_mask(indx_val, labels.shape[0])\n",
    "    test_mask_ = sample_mask(indx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask_, :] = labels[train_mask_, :]\n",
    "    y_val[valid_mask_, :] = labels[valid_mask_, :]\n",
    "    y_test[test_mask_, :] = labels[test_mask_, :]\n",
    "\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask_, valid_mask_, test_mask_, train_size, test_size\n",
    "\n",
    "\n",
    "#converting sparse to tuple\n",
    "def sparse_to_tuple(sparse_mat):\n",
    "    def to_tuple(mat):\n",
    "        if not sp.isspmatrix_coo(mat):\n",
    "            mat = mat.tocoo()\n",
    "        coords = np.vstack((mat.row, mat.col)).transpose()\n",
    "        vals = mat.data\n",
    "        shape = mat.shape\n",
    "        return coords, vals, shape\n",
    "\n",
    "    if isinstance(sparse_mat, list):\n",
    "        for i in range(len(sparse_mat)):\n",
    "            sparse_mat[i] = to_tuple(sparse_mat[i])\n",
    "    else:\n",
    "        sparse_mat = to_tuple(sparse_mat)\n",
    "\n",
    "    return sparse_mat\n",
    "\n",
    "\n",
    "#features preprocessing\n",
    "def preprocess_features(features_):\n",
    "    row_sum = np.array(features_.sum(1))\n",
    "    r_inver = np.power(row_sum, -1).flatten()\n",
    "    r_inver[np.isinf(r_inver)] = 0.\n",
    "    r_mat_inver = sp.diags(r_inver)\n",
    "    features_ = r_mat_inver.dot(features_)\n",
    "    \n",
    "    return sparse_to_tuple(features_)\n",
    "\n",
    "\n",
    "#normalization\n",
    "def normalize_adj(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    row_sum = np.array(adj.sum(1))\n",
    "    d_inver_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "    d_inver_sqrt[np.isinf(d_inver_sqrt)] = 0.\n",
    "    d_mat_inver_sqrt = sp.diags(d_inver_sqrt)\n",
    "    \n",
    "    return adj.dot(d_mat_inver_sqrt).transpose().dot(d_mat_inver_sqrt).tocoo()\n",
    "\n",
    "#preprocess adj mat\n",
    "def preprocess_adj(adj_mat):\n",
    "    adjmat_normalized = normalize_adj(adj_mat + sp.eye(adj_mat.shape[0]))  \n",
    "    return sparse_to_tuple(adjmat_normalized)\n",
    "\n",
    "\n",
    "#feed dict\n",
    "def construct_feed_dict(features_, support, labels, labels_mask, placeholder):\n",
    "    feed_dict_ = dict()\n",
    "    feed_dict_.update({placeholder['labels']: labels})\n",
    "    feed_dict_.update({placeholder['labels_mask']: labels_mask})\n",
    "    feed_dict_.update({placeholder['features']: features_})\n",
    "    feed_dict_.update({placeholder['support'][i]: support[i]\n",
    "                      for i in range(len(support))})\n",
    "    feed_dict_.update({placeholder['num_features_nonzero']: features_[1].shape})\n",
    "    return feed_dict_\n",
    "\n",
    "#loading word2vec\n",
    "def loadWord2Vec(file_name):\n",
    "    vocab = []\n",
    "    embed = []\n",
    "    word2vector_map = {}\n",
    "    file = open(file_name, 'r')\n",
    "    for l in file.readlines():\n",
    "        row = l.strip().split(' ')\n",
    "        if(len(row) > 2):\n",
    "            vocab.append(row[0])\n",
    "            vector = row[1:]\n",
    "            length = len(vector)\n",
    "            for i in range(length):\n",
    "                vector[i] = float(vector[i])\n",
    "            embed.append(vector)\n",
    "            word2vector_map[row[0]] = vector\n",
    "    \n",
    "    print('Loaded all Word-Vectors!')\n",
    "    file.close()\n",
    "    return vocab, embed, word2vector_map\n",
    "\n",
    "#cleasing of str\n",
    "def clean_str(st):\n",
    "    st = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", st)\n",
    "    st = re.sub(r\"\\'s\", \" \\'s\", st)\n",
    "    st = re.sub(r\"\\'ve\", \" \\'ve\", st)\n",
    "    st = re.sub(r\"n\\'t\", \" n\\'t\", st)\n",
    "    st = re.sub(r\"\\'re\", \" \\'re\", st)\n",
    "    st = re.sub(r\"\\'d\", \" \\'d\", st)\n",
    "    st = re.sub(r\"\\'ll\", \" \\'ll\", st)\n",
    "    st = re.sub(r\",\", \" , \", st)\n",
    "    st = re.sub(r\"!\", \" ! \", st)\n",
    "    st = re.sub(r\"\\(\", \" \\( \", st)\n",
    "    st = re.sub(r\"\\)\", \" \\) \", st)\n",
    "    st = re.sub(r\"\\?\", \" \\? \", st)\n",
    "    st = re.sub(r\"\\s{2,}\", \" \", st)\n",
    "    return st.strip().lower()\n",
    "\n",
    "def uniform(shape, scale=0.05, name=None):\n",
    "    init = tf.random.uniform(shape, minval=-scale, maxval=scale, dtype=tf.float64)\n",
    "    return tf.Variable(init, name=name)\n",
    "\n",
    "\n",
    "def glorot(shape, name=None):\n",
    "    initial_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    init = tf.random.uniform(shape, minval=-initial_range, maxval=initial_range, dtype=tf.float64)\n",
    "    return tf.Variable(init, name=name)\n",
    "\n",
    "\n",
    "def zeros(shape, name=None):\n",
    "    init = tf.zeros(shape, dtype=tf.float64)\n",
    "    return tf.Variable(init, name=name)\n",
    "\n",
    "\n",
    "def ones(shape, name=None):\n",
    "    init = tf.ones(shape, dtype=tf.float64)\n",
    "    return tf.Variable(init, name=name)\n",
    "\n",
    "\n",
    "\n",
    "_LAYER_IDS = {}\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_IDS:\n",
    "        _LAYER_IDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_IDS[layer_name] += 1\n",
    "        return _LAYER_IDS[layer_name]\n",
    "\n",
    "\n",
    "def sparse_dropout(X, rate, noise_shape):\n",
    "    random_tens = rate\n",
    "    random_tens += tf.random.uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tens), dtype=tf.bool)\n",
    "    pre_out = tf.sparse.retain(X, dropout_mask)\n",
    "    return pre_out * (1./(rate))\n",
    "\n",
    "\n",
    "def dot(X, y, sparse=False):\n",
    "    if sparse:\n",
    "        result = tf.sparse.sparse_dense_matmul(X, y)\n",
    "    else:\n",
    "        result = tf.matmul(X, y)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7fd9541",
   "metadata": {
    "id": "e7fd9541"
   },
   "outputs": [],
   "source": [
    "#gettinf training and testing sets\n",
    "adj_mat, features_, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(dataset)\n",
    "\n",
    "features_ = sp.identity(features_.shape[0])  \n",
    "features_ = preprocess_features(features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0af0acef",
   "metadata": {
    "id": "0af0acef"
   },
   "outputs": [],
   "source": [
    "support = [preprocess_adj(adj_mat)]\n",
    "\n",
    "t_features = tf.SparseTensor(*features_)\n",
    "ty_train = tf.convert_to_tensor(y_train)\n",
    "ty_val = tf.convert_to_tensor(y_val)\n",
    "ty_test = tf.convert_to_tensor(y_test)\n",
    "tm_train_mask = tf.convert_to_tensor(train_mask)\n",
    "\n",
    "tm_val_mask = tf.convert_to_tensor(val_mask)\n",
    "tm_test_mask = tf.convert_to_tensor(test_mask)\n",
    "\n",
    "t_support = []\n",
    "for i in range(len(support)):\n",
    "    t_support.append(tf.cast(tf.SparseTensor(*support[i]), dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a3ca1ff",
   "metadata": {
    "id": "4a3ca1ff"
   },
   "outputs": [],
   "source": [
    "#GCN\n",
    "class GraphConvolution(layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero,\n",
    "                 dropout=0.,\n",
    "                 is_sparse_inputs=False,\n",
    "                 activation=tf.nn.relu,\n",
    "                 bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.is_sparse_inputs = is_sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.num_features_nonzero = num_features_nonzero\n",
    "        self.embedding = None\n",
    "\n",
    "        self.weights_ = []\n",
    "        for i in range(1):\n",
    "            w = self.add_variable('weight' + str(i), [input_dim, output_dim])\n",
    "            self.weights_.append(w)\n",
    "        if self.bias:\n",
    "            self.bias = self.add_variable('bias', [output_dim])\n",
    "\n",
    "\n",
    "\n",
    "    #calling network\n",
    "    def call(self, inputs, training=None):\n",
    "        X, support_ = inputs\n",
    "\n",
    "        # dropout\n",
    "        if training is not False and self.is_sparse_inputs:\n",
    "            X = sparse_dropout(X, self.dropout, self.num_features_nonzero)\n",
    "        elif training is not False:\n",
    "            X = tf.nn.dropout(X, self.dropout)\n",
    "\n",
    "\n",
    "        # convolving\n",
    "        supports = list()\n",
    "        for i in range(len(support_)):\n",
    "            if not self.featureless: \n",
    "                pre_sup = dot(X, self.weights_[i], sparse=self.is_sparse_inputs)\n",
    "            else:\n",
    "                pre_sup = self.weights_[i]\n",
    "\n",
    "            support = dot(support_[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        if self.bias:\n",
    "            output += self.bias\n",
    "        \n",
    "        self.embedding = output \n",
    "        return self.activation(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36f40c8e",
   "metadata": {
    "id": "36f40c8e"
   },
   "outputs": [],
   "source": [
    "class GCN(keras.Model):\n",
    "\n",
    "    def __init__(self, input_dimen, output_dimen, num_features_nonzero, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_dimen= input_dimen # 1433\n",
    "        self.output_dimen = output_dimen\n",
    "\n",
    "        print('input  dimension: ', input_dimen)\n",
    "        print('output dimension: ', output_dimen)\n",
    "\n",
    "        self.layers_ = []\n",
    "        self.layers_.append(GraphConvolution(input_dim=self.input_dimen,\n",
    "                                            output_dim=hidden1, \n",
    "                                            num_features_nonzero=num_features_nonzero,\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            dropout=dropout,\n",
    "                                            is_sparse_inputs=True))\n",
    "\n",
    "\n",
    "        self.layers_.append(GraphConvolution(input_dim=hidden1, \n",
    "                                            output_dim=self.output_dimen, \n",
    "                                            num_features_nonzero=num_features_nonzero,\n",
    "                                            activation=lambda X: X,\n",
    "                                            dropout=dropout))\n",
    "\n",
    "\n",
    "            \n",
    "    def call(self, inputs, training=None):\n",
    "        X, label, mask, support = inputs\n",
    "\n",
    "        outputs = [X]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden = layer((outputs[-1], support), training)\n",
    "            outputs.append(hidden)\n",
    "        output = outputs[-1]\n",
    "\n",
    "        # # Weight decay loss\n",
    "        loss = tf.zeros([])\n",
    "        for var in self.layers_[0].trainable_variables:\n",
    "            loss +=weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        loss += masked_softmax_cross_entropy(output, label, mask)\n",
    "\n",
    "        acc = masked_accuracy(output, label, mask)\n",
    "\n",
    "        return tf.argmax(output, 1), loss, acc\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0e48246",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1632576232214,
     "user": {
      "displayName": "Palak techies",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04966026704557831019"
     },
     "user_tz": -330
    },
    "id": "c0e48246",
    "outputId": "df7520fc-4ebe-4a0d-c50b-0e4c0b8a3edd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  dimension:  30098\n",
      "output dimension:  3\n"
     ]
    }
   ],
   "source": [
    "# Creating model\n",
    "model = GCN(input_dimen=features_[2][1], output_dimen=y_train.shape[1], num_features_nonzero=features_[1].shape)\n",
    "\n",
    "# Loss and optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "cost_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d57a99e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 238164,
     "status": "ok",
     "timestamp": 1632576532410,
     "user": {
      "displayName": "Palak techies",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04966026704557831019"
     },
     "user_tz": -330
    },
    "id": "0d57a99e",
    "outputId": "bee1a428-41a8-45de-abe9-4633df3feb83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gcn_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch: 0001 train_loss= 1.09856 train_acc= 0.37489 val_loss= 1.09302 val_acc= 0.62493 time= 1.88882\n",
      "Epoch: 0002 train_loss= 1.09295 train_acc= 0.62127 val_loss= 1.08064 val_acc= 0.60922 time= 1.89498\n",
      "Epoch: 0003 train_loss= 1.08037 train_acc= 0.61598 val_loss= 1.06044 val_acc= 0.59298 time= 1.88714\n",
      "Epoch: 0004 train_loss= 1.06033 train_acc= 0.59672 val_loss= 1.03242 val_acc= 0.59298 time= 1.85093\n",
      "Epoch: 0005 train_loss= 1.03271 train_acc= 0.60463 val_loss= 0.99752 val_acc= 0.60084 time= 1.89622\n",
      "Epoch: 0006 train_loss= 0.99691 train_acc= 0.60207 val_loss= 0.95754 val_acc= 0.60450 time= 1.88909\n",
      "Epoch: 0007 train_loss= 0.95694 train_acc= 0.60754 val_loss= 0.91525 val_acc= 0.60503 time= 1.90136\n",
      "Epoch: 0008 train_loss= 0.91243 train_acc= 0.60923 val_loss= 0.87382 val_acc= 0.60817 time= 1.90247\n",
      "Epoch: 0009 train_loss= 0.87150 train_acc= 0.61237 val_loss= 0.83638 val_acc= 0.61027 time= 1.89895\n",
      "Epoch: 0010 train_loss= 0.83115 train_acc= 0.61382 val_loss= 0.80434 val_acc= 0.62074 time= 1.85970\n",
      "Epoch: 0011 train_loss= 0.80045 train_acc= 0.62098 val_loss= 0.77629 val_acc= 0.63174 time= 1.89736\n",
      "Epoch: 0012 train_loss= 0.77045 train_acc= 0.63989 val_loss= 0.74951 val_acc= 0.65898 time= 1.87337\n",
      "Epoch: 0013 train_loss= 0.74566 train_acc= 0.65216 val_loss= 0.72221 val_acc= 0.66370 time= 1.88117\n",
      "Epoch: 0014 train_loss= 0.71732 train_acc= 0.66170 val_loss= 0.69417 val_acc= 0.67889 time= 1.89074\n",
      "Epoch: 0015 train_loss= 0.69090 train_acc= 0.68154 val_loss= 0.66632 val_acc= 0.69984 time= 1.90099\n",
      "Epoch: 0016 train_loss= 0.66173 train_acc= 0.70155 val_loss= 0.64026 val_acc= 0.71032 time= 1.89339\n",
      "Epoch: 0017 train_loss= 0.63754 train_acc= 0.71371 val_loss= 0.61717 val_acc= 0.72603 time= 1.90121\n",
      "Epoch: 0018 train_loss= 0.61057 train_acc= 0.73244 val_loss= 0.59738 val_acc= 0.73913 time= 1.87510\n",
      "Epoch: 0019 train_loss= 0.59497 train_acc= 0.74641 val_loss= 0.58063 val_acc= 0.75223 time= 1.88113\n",
      "Epoch: 0020 train_loss= 0.57446 train_acc= 0.75833 val_loss= 0.56646 val_acc= 0.75694 time= 1.88858\n",
      "Epoch: 0021 train_loss= 0.55757 train_acc= 0.76712 val_loss= 0.55455 val_acc= 0.76532 time= 1.90004\n",
      "Epoch: 0022 train_loss= 0.54664 train_acc= 0.77294 val_loss= 0.54443 val_acc= 0.76532 time= 1.89167\n",
      "Epoch: 0023 train_loss= 0.53369 train_acc= 0.78073 val_loss= 0.53558 val_acc= 0.76427 time= 1.89940\n",
      "Epoch: 0024 train_loss= 0.52150 train_acc= 0.78585 val_loss= 0.52749 val_acc= 0.76218 time= 1.89199\n",
      "Epoch: 0025 train_loss= 0.51273 train_acc= 0.78248 val_loss= 0.51914 val_acc= 0.76480 time= 1.89818\n",
      "Epoch: 0026 train_loss= 0.49995 train_acc= 0.78893 val_loss= 0.51070 val_acc= 0.76951 time= 1.91997\n",
      "Epoch: 0027 train_loss= 0.48857 train_acc= 0.78998 val_loss= 0.50240 val_acc= 0.77056 time= 1.90564\n",
      "Epoch: 0028 train_loss= 0.47736 train_acc= 0.79109 val_loss= 0.49467 val_acc= 0.77632 time= 1.92958\n",
      "Epoch: 0029 train_loss= 0.47071 train_acc= 0.78998 val_loss= 0.48755 val_acc= 0.77999 time= 1.89315\n",
      "Epoch: 0030 train_loss= 0.46044 train_acc= 0.79463 val_loss= 0.48148 val_acc= 0.78785 time= 1.87022\n",
      "Epoch: 0031 train_loss= 0.45464 train_acc= 0.79946 val_loss= 0.47625 val_acc= 0.78942 time= 1.92455\n",
      "Epoch: 0032 train_loss= 0.44890 train_acc= 0.80336 val_loss= 0.47100 val_acc= 0.79308 time= 1.91928\n",
      "Epoch: 0033 train_loss= 0.43915 train_acc= 0.80726 val_loss= 0.46565 val_acc= 0.79361 time= 1.91096\n",
      "Epoch: 0034 train_loss= 0.43286 train_acc= 0.81366 val_loss= 0.46073 val_acc= 0.79832 time= 1.88937\n",
      "Epoch: 0035 train_loss= 0.42586 train_acc= 0.81872 val_loss= 0.45702 val_acc= 0.80199 time= 1.88124\n",
      "Epoch: 0036 train_loss= 0.41703 train_acc= 0.82407 val_loss= 0.45475 val_acc= 0.79989 time= 1.88642\n",
      "Epoch: 0037 train_loss= 0.41281 train_acc= 0.82372 val_loss= 0.45256 val_acc= 0.79989 time= 1.87931\n",
      "Epoch: 0038 train_loss= 0.40911 train_acc= 0.82465 val_loss= 0.44904 val_acc= 0.80356 time= 1.88394\n",
      "Epoch: 0039 train_loss= 0.40301 train_acc= 0.83012 val_loss= 0.44433 val_acc= 0.80775 time= 1.88519\n",
      "Epoch: 0040 train_loss= 0.39267 train_acc= 0.83559 val_loss= 0.43958 val_acc= 0.81142 time= 1.89534\n",
      "Epoch: 0041 train_loss= 0.38839 train_acc= 0.84083 val_loss= 0.43536 val_acc= 0.81456 time= 1.88217\n",
      "Epoch: 0042 train_loss= 0.37904 train_acc= 0.84280 val_loss= 0.43212 val_acc= 0.81613 time= 1.86788\n",
      "Epoch: 0043 train_loss= 0.37592 train_acc= 0.84135 val_loss= 0.42939 val_acc= 0.81456 time= 1.89011\n",
      "Epoch: 0044 train_loss= 0.37482 train_acc= 0.84042 val_loss= 0.42675 val_acc= 0.81718 time= 1.91020\n",
      "Epoch: 0045 train_loss= 0.36678 train_acc= 0.84298 val_loss= 0.42390 val_acc= 0.82347 time= 1.91019\n",
      "Epoch: 0046 train_loss= 0.36133 train_acc= 0.84926 val_loss= 0.42107 val_acc= 0.82451 time= 1.91548\n",
      "Epoch: 0047 train_loss= 0.35776 train_acc= 0.84961 val_loss= 0.41837 val_acc= 0.82504 time= 1.89667\n",
      "Epoch: 0048 train_loss= 0.34833 train_acc= 0.86090 val_loss= 0.41646 val_acc= 0.82399 time= 1.88599\n",
      "Epoch: 0049 train_loss= 0.34748 train_acc= 0.86084 val_loss= 0.41499 val_acc= 0.82294 time= 1.89625\n",
      "Epoch: 0050 train_loss= 0.34384 train_acc= 0.86334 val_loss= 0.41455 val_acc= 0.82556 time= 1.88932\n",
      "Epoch: 0051 train_loss= 0.33580 train_acc= 0.86276 val_loss= 0.41378 val_acc= 0.82923 time= 1.88097\n",
      "Epoch: 0052 train_loss= 0.33595 train_acc= 0.86322 val_loss= 0.41252 val_acc= 0.82923 time= 1.90074\n",
      "Epoch: 0053 train_loss= 0.33105 train_acc= 0.86375 val_loss= 0.40931 val_acc= 0.82818 time= 1.87686\n",
      "Epoch: 0054 train_loss= 0.32570 train_acc= 0.86648 val_loss= 0.40542 val_acc= 0.83132 time= 1.88471\n",
      "Epoch: 0055 train_loss= 0.32517 train_acc= 0.86706 val_loss= 0.40308 val_acc= 0.83552 time= 1.89389\n",
      "Epoch: 0056 train_loss= 0.32250 train_acc= 0.87346 val_loss= 0.40185 val_acc= 0.83656 time= 1.88369\n",
      "Epoch: 0057 train_loss= 0.31557 train_acc= 0.87300 val_loss= 0.40020 val_acc= 0.84023 time= 1.90011\n",
      "Epoch: 0058 train_loss= 0.31136 train_acc= 0.87707 val_loss= 0.39803 val_acc= 0.83971 time= 1.90122\n",
      "Epoch: 0059 train_loss= 0.30640 train_acc= 0.88114 val_loss= 0.39630 val_acc= 0.84023 time= 1.87811\n",
      "Epoch: 0060 train_loss= 0.30401 train_acc= 0.88376 val_loss= 0.39536 val_acc= 0.83971 time= 1.89819\n",
      "Epoch: 0061 train_loss= 0.30370 train_acc= 0.88213 val_loss= 0.39596 val_acc= 0.83918 time= 1.90897\n",
      "Epoch: 0062 train_loss= 0.29885 train_acc= 0.88434 val_loss= 0.39625 val_acc= 0.84075 time= 1.93431\n",
      "Epoch: 0063 train_loss= 0.29842 train_acc= 0.88830 val_loss= 0.39633 val_acc= 0.84390 time= 1.91661\n",
      "Epoch: 0064 train_loss= 0.29603 train_acc= 0.88248 val_loss= 0.39279 val_acc= 0.84599 time= 1.90761\n",
      "Epoch: 0065 train_loss= 0.28705 train_acc= 0.88353 val_loss= 0.38860 val_acc= 0.84756 time= 1.94764\n",
      "Epoch: 0066 train_loss= 0.28279 train_acc= 0.88894 val_loss= 0.38766 val_acc= 0.85228 time= 1.96051\n",
      "Epoch: 0067 train_loss= 0.29120 train_acc= 0.88766 val_loss= 0.38673 val_acc= 0.85123 time= 1.92626\n",
      "Epoch: 0068 train_loss= 0.28114 train_acc= 0.89289 val_loss= 0.38499 val_acc= 0.85333 time= 1.90271\n",
      "Epoch: 0069 train_loss= 0.27609 train_acc= 0.89359 val_loss= 0.38463 val_acc= 0.84914 time= 1.94141\n",
      "Epoch: 0070 train_loss= 0.27477 train_acc= 0.89481 val_loss= 0.38654 val_acc= 0.84547 time= 1.89050\n",
      "Epoch: 0071 train_loss= 0.27581 train_acc= 0.89103 val_loss= 0.38774 val_acc= 0.84756 time= 1.93086\n",
      "Epoch: 0072 train_loss= 0.26778 train_acc= 0.89604 val_loss= 0.38649 val_acc= 0.84652 time= 1.87921\n",
      "Epoch: 0073 train_loss= 0.27077 train_acc= 0.89615 val_loss= 0.38288 val_acc= 0.84966 time= 1.88598\n",
      "Epoch: 0074 train_loss= 0.26526 train_acc= 0.89825 val_loss= 0.38096 val_acc= 0.85228 time= 1.90848\n",
      "Epoch: 0075 train_loss= 0.26249 train_acc= 0.89836 val_loss= 0.38048 val_acc= 0.85490 time= 1.90266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0076 train_loss= 0.25853 train_acc= 0.90203 val_loss= 0.38041 val_acc= 0.85752 time= 1.89829\n",
      "Epoch: 0077 train_loss= 0.25431 train_acc= 0.90110 val_loss= 0.37980 val_acc= 0.85804 time= 1.88858\n",
      "Epoch: 0078 train_loss= 0.25534 train_acc= 0.90028 val_loss= 0.37850 val_acc= 0.85385 time= 1.92630\n",
      "Epoch: 0079 train_loss= 0.25178 train_acc= 0.90447 val_loss= 0.37712 val_acc= 0.85437 time= 1.88840\n",
      "Epoch: 0080 train_loss= 0.25168 train_acc= 0.90505 val_loss= 0.37690 val_acc= 0.85175 time= 1.88400\n",
      "Epoch: 0081 train_loss= 0.25068 train_acc= 0.90703 val_loss= 0.37630 val_acc= 0.85228 time= 1.87438\n",
      "Epoch: 0082 train_loss= 0.24421 train_acc= 0.90936 val_loss= 0.37556 val_acc= 0.85437 time= 1.88220\n",
      "Epoch: 0083 train_loss= 0.24449 train_acc= 0.90872 val_loss= 0.37474 val_acc= 0.85542 time= 1.87535\n",
      "Epoch: 0084 train_loss= 0.24211 train_acc= 0.91192 val_loss= 0.37461 val_acc= 0.85437 time= 1.91616\n",
      "Epoch: 0085 train_loss= 0.24310 train_acc= 0.90901 val_loss= 0.37826 val_acc= 0.85490 time= 1.89393\n",
      "Epoch: 0086 train_loss= 0.24325 train_acc= 0.90587 val_loss= 0.38030 val_acc= 0.85647 time= 1.91632\n",
      "Epoch: 0087 train_loss= 0.23974 train_acc= 0.90651 val_loss= 0.37775 val_acc= 0.85752 time= 1.87795\n",
      "Epoch: 0088 train_loss= 0.23885 train_acc= 0.91017 val_loss= 0.37458 val_acc= 0.85856 time= 1.89997\n",
      "Epoch: 0089 train_loss= 0.22930 train_acc= 0.91360 val_loss= 0.37490 val_acc= 0.85909 time= 1.89786\n",
      "Epoch: 0090 train_loss= 0.23178 train_acc= 0.91407 val_loss= 0.37541 val_acc= 0.85961 time= 1.89147\n",
      "Epoch: 0091 train_loss= 0.23526 train_acc= 0.91331 val_loss= 0.37473 val_acc= 0.85752 time= 1.87733\n",
      "Epoch: 0092 train_loss= 0.22808 train_acc= 0.91599 val_loss= 0.37587 val_acc= 0.85856 time= 1.89953\n",
      "Epoch: 0093 train_loss= 0.22831 train_acc= 0.91331 val_loss= 0.37979 val_acc= 0.85961 time= 1.89453\n",
      "Epoch: 0094 train_loss= 0.23234 train_acc= 0.90982 val_loss= 0.37948 val_acc= 0.86223 time= 1.89912\n",
      "Epoch: 0095 train_loss= 0.22482 train_acc= 0.91366 val_loss= 0.37642 val_acc= 0.86433 time= 1.92579\n",
      "Epoch: 0096 train_loss= 0.22074 train_acc= 0.91733 val_loss= 0.37493 val_acc= 0.86118 time= 1.90835\n",
      "Epoch: 0097 train_loss= 0.22424 train_acc= 0.91442 val_loss= 0.37563 val_acc= 0.86171 time= 1.91237\n",
      "Epoch: 0098 train_loss= 0.22802 train_acc= 0.91198 val_loss= 0.37486 val_acc= 0.86066 time= 1.87592\n",
      "Epoch: 0099 train_loss= 0.21883 train_acc= 0.91872 val_loss= 0.37384 val_acc= 0.86328 time= 1.91395\n",
      "Epoch: 0100 train_loss= 0.22579 train_acc= 0.91843 val_loss= 0.37435 val_acc= 0.86328 time= 1.89239\n",
      "Epoch: 0101 train_loss= 0.21926 train_acc= 0.91878 val_loss= 0.37867 val_acc= 0.86328 time= 1.91625\n",
      "Epoch: 0102 train_loss= 0.22195 train_acc= 0.91436 val_loss= 0.37956 val_acc= 0.86380 time= 1.88556\n",
      "Epoch: 0103 train_loss= 0.21601 train_acc= 0.91942 val_loss= 0.37922 val_acc= 0.86328 time= 1.88585\n",
      "Epoch: 0104 train_loss= 0.21966 train_acc= 0.91512 val_loss= 0.37606 val_acc= 0.86328 time= 1.87823\n",
      "Epoch: 0105 train_loss= 0.21426 train_acc= 0.91936 val_loss= 0.37422 val_acc= 0.86380 time= 1.93028\n",
      "Epoch: 0106 train_loss= 0.21072 train_acc= 0.92338 val_loss= 0.37448 val_acc= 0.86223 time= 1.89510\n",
      "Epoch: 0107 train_loss= 0.21734 train_acc= 0.92024 val_loss= 0.37562 val_acc= 0.85490 time= 1.89224\n",
      "Epoch: 0108 train_loss= 0.20606 train_acc= 0.92483 val_loss= 0.37651 val_acc= 0.85804 time= 1.90235\n",
      "Epoch: 0109 train_loss= 0.20953 train_acc= 0.92355 val_loss= 0.37642 val_acc= 0.86223 time= 1.91354\n",
      "Epoch: 0110 train_loss= 0.20404 train_acc= 0.92379 val_loss= 0.37822 val_acc= 0.86537 time= 1.90099\n",
      "Epoch: 0111 train_loss= 0.20669 train_acc= 0.92251 val_loss= 0.37919 val_acc= 0.86485 time= 1.89406\n",
      "Epoch: 0112 train_loss= 0.20448 train_acc= 0.92349 val_loss= 0.37973 val_acc= 0.86590 time= 1.87158\n",
      "Epoch: 0113 train_loss= 0.20316 train_acc= 0.92477 val_loss= 0.37797 val_acc= 0.86747 time= 1.89116\n",
      "Epoch: 0114 train_loss= 0.19913 train_acc= 0.92786 val_loss= 0.37691 val_acc= 0.86695 time= 1.89975\n",
      "Epoch: 0115 train_loss= 0.20050 train_acc= 0.92280 val_loss= 0.37625 val_acc= 0.86171 time= 1.90811\n",
      "Epoch: 0116 train_loss= 0.19938 train_acc= 0.92384 val_loss= 0.37893 val_acc= 0.85961 time= 1.88477\n",
      "Epoch: 0117 train_loss= 0.19703 train_acc= 0.92914 val_loss= 0.38337 val_acc= 0.85542 time= 1.87828\n",
      "Epoch: 0118 train_loss= 0.20526 train_acc= 0.92262 val_loss= 0.38404 val_acc= 0.85699 time= 1.88519\n",
      "Epoch: 0119 train_loss= 0.19531 train_acc= 0.93042 val_loss= 0.38391 val_acc= 0.86118 time= 1.87658\n",
      "Epoch: 0120 train_loss= 0.19898 train_acc= 0.92664 val_loss= 0.38156 val_acc= 0.86328 time= 1.88414\n",
      "Epoch: 0121 train_loss= 0.19362 train_acc= 0.92658 val_loss= 0.38026 val_acc= 0.86852 time= 1.90842\n",
      "Epoch: 0122 train_loss= 0.18947 train_acc= 0.93344 val_loss= 0.38364 val_acc= 0.86747 time= 1.88974\n",
      "Epoch: 0123 train_loss= 0.19460 train_acc= 0.92640 val_loss= 0.38617 val_acc= 0.86328 time= 1.90588\n",
      "Epoch: 0124 train_loss= 0.19899 train_acc= 0.92646 val_loss= 0.38448 val_acc= 0.86799 time= 1.87025\n",
      "Epoch: 0125 train_loss= 0.19499 train_acc= 0.92879 val_loss= 0.38313 val_acc= 0.86642 time= 1.89192\n",
      "Epoch: 0126 train_loss= 0.19165 train_acc= 0.92978 val_loss= 0.38798 val_acc= 0.86642 time= 1.89999\n",
      "Epoch: 0127 train_loss= 0.18992 train_acc= 0.92827 val_loss= 0.39135 val_acc= 0.86275 time= 1.87661\n",
      "Epoch: 0128 train_loss= 0.19791 train_acc= 0.92233 val_loss= 0.38793 val_acc= 0.86537 time= 1.89975\n",
      "Epoch: 0129 train_loss= 0.19763 train_acc= 0.92617 val_loss= 0.38418 val_acc= 0.86380 time= 1.92342\n",
      "Epoch: 0130 train_loss= 0.19323 train_acc= 0.92605 val_loss= 0.38475 val_acc= 0.86956 time= 1.87369\n",
      "Epoch: 0131 train_loss= 0.18746 train_acc= 0.92896 val_loss= 0.39173 val_acc= 0.86223 time= 1.91294\n",
      "Epoch: 0132 train_loss= 0.19812 train_acc= 0.92600 val_loss= 0.39439 val_acc= 0.86223 time= 1.90545\n",
      "Epoch: 0133 train_loss= 0.19106 train_acc= 0.92774 val_loss= 0.38982 val_acc= 0.86380 time= 1.91430\n",
      "Epoch: 0134 train_loss= 0.19639 train_acc= 0.92850 val_loss= 0.38481 val_acc= 0.86956 time= 1.93986\n",
      "Epoch: 0135 train_loss= 0.18667 train_acc= 0.93048 val_loss= 0.39132 val_acc= 0.86485 time= 1.88604\n",
      "Epoch: 0136 train_loss= 0.18407 train_acc= 0.93216 val_loss= 0.40267 val_acc= 0.85856 time= 1.89527\n",
      "Epoch: 0137 train_loss= 0.19339 train_acc= 0.92844 val_loss= 0.40933 val_acc= 0.85909 time= 1.91958\n",
      "Epoch: 0138 train_loss= 0.19913 train_acc= 0.92361 val_loss= 0.40357 val_acc= 0.86171 time= 1.89715\n",
      "Epoch: 0139 train_loss= 0.18710 train_acc= 0.93018 val_loss= 0.39346 val_acc= 0.86747 time= 1.90541\n",
      "Epoch: 0140 train_loss= 0.18212 train_acc= 0.93129 val_loss= 0.38855 val_acc= 0.87218 time= 1.88505\n",
      "Epoch: 0141 train_loss= 0.17670 train_acc= 0.93676 val_loss= 0.39419 val_acc= 0.86799 time= 1.92265\n",
      "Epoch: 0142 train_loss= 0.19153 train_acc= 0.92745 val_loss= 0.39848 val_acc= 0.86066 time= 1.88677\n",
      "Epoch: 0143 train_loss= 0.19093 train_acc= 0.92809 val_loss= 0.39494 val_acc= 0.86328 time= 1.88014\n",
      "Epoch: 0144 train_loss= 0.19634 train_acc= 0.92553 val_loss= 0.38965 val_acc= 0.86956 time= 1.90175\n",
      "Epoch: 0145 train_loss= 0.17444 train_acc= 0.93688 val_loss= 0.39711 val_acc= 0.86590 time= 1.89328\n",
      "Epoch: 0146 train_loss= 0.18350 train_acc= 0.93059 val_loss= 0.40722 val_acc= 0.86433 time= 1.88619\n",
      "Epoch: 0147 train_loss= 0.19602 train_acc= 0.92384 val_loss= 0.40508 val_acc= 0.86433 time= 1.89343\n",
      "Epoch: 0148 train_loss= 0.18281 train_acc= 0.93001 val_loss= 0.39697 val_acc= 0.86433 time= 1.88497\n",
      "Epoch: 0149 train_loss= 0.18687 train_acc= 0.92984 val_loss= 0.39180 val_acc= 0.87009 time= 1.89429\n",
      "Epoch: 0150 train_loss= 0.17428 train_acc= 0.93664 val_loss= 0.39323 val_acc= 0.86852 time= 1.90020\n",
      "Epoch: 0151 train_loss= 0.17817 train_acc= 0.93397 val_loss= 0.39685 val_acc= 0.86747 time= 1.89595\n",
      "Epoch: 0152 train_loss= 0.18581 train_acc= 0.93129 val_loss= 0.40123 val_acc= 0.87009 time= 1.90422\n",
      "Epoch: 0153 train_loss= 0.18194 train_acc= 0.93152 val_loss= 0.40831 val_acc= 0.86537 time= 1.89592\n",
      "Epoch: 0154 train_loss= 0.17160 train_acc= 0.93635 val_loss= 0.41248 val_acc= 0.86799 time= 1.91115\n",
      "Epoch: 0155 train_loss= 0.18788 train_acc= 0.92594 val_loss= 0.40409 val_acc= 0.86904 time= 1.89583\n",
      "Epoch: 0156 train_loss= 0.17677 train_acc= 0.93338 val_loss= 0.39610 val_acc= 0.87061 time= 1.92366\n",
      "Epoch: 0157 train_loss= 0.17364 train_acc= 0.93327 val_loss= 0.39472 val_acc= 0.86799 time= 1.88571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0158 train_loss= 0.17877 train_acc= 0.93373 val_loss= 0.39737 val_acc= 0.86275 time= 1.86561\n",
      "Epoch: 0159 train_loss= 0.17803 train_acc= 0.93519 val_loss= 0.39612 val_acc= 0.86433 time= 1.87159\n",
      "Epoch: 0160 train_loss= 0.18069 train_acc= 0.93356 val_loss= 0.39377 val_acc= 0.86956 time= 1.86645\n",
      "Epoch: 0161 train_loss= 0.16990 train_acc= 0.93775 val_loss= 0.39624 val_acc= 0.87218 time= 1.88852\n",
      "Epoch: 0162 train_loss= 0.17226 train_acc= 0.93676 val_loss= 0.39972 val_acc= 0.87114 time= 1.91277\n",
      "Epoch: 0163 train_loss= 0.17375 train_acc= 0.93373 val_loss= 0.40028 val_acc= 0.87009 time= 1.90353\n",
      "Epoch: 0164 train_loss= 0.16530 train_acc= 0.93897 val_loss= 0.39890 val_acc= 0.87218 time= 1.89860\n",
      "Epoch: 0165 train_loss= 0.17352 train_acc= 0.93606 val_loss= 0.39653 val_acc= 0.87690 time= 1.93623\n",
      "Epoch: 0166 train_loss= 0.16411 train_acc= 0.93909 val_loss= 0.39532 val_acc= 0.87218 time= 1.93513\n",
      "Epoch: 0167 train_loss= 0.16025 train_acc= 0.94205 val_loss= 0.39615 val_acc= 0.86956 time= 1.91364\n",
      "Epoch: 0168 train_loss= 0.16555 train_acc= 0.93973 val_loss= 0.39901 val_acc= 0.87009 time= 1.90424\n",
      "Epoch: 0169 train_loss= 0.16728 train_acc= 0.93757 val_loss= 0.40170 val_acc= 0.86747 time= 1.89721\n",
      "Epoch: 0170 train_loss= 0.17187 train_acc= 0.93821 val_loss= 0.40136 val_acc= 0.86852 time= 1.95451\n",
      "Epoch: 0171 train_loss= 0.16843 train_acc= 0.93658 val_loss= 0.39911 val_acc= 0.86852 time= 1.92646\n",
      "Epoch: 0172 train_loss= 0.16445 train_acc= 0.94165 val_loss= 0.39836 val_acc= 0.87218 time= 1.94395\n",
      "Epoch: 0173 train_loss= 0.16394 train_acc= 0.93996 val_loss= 0.40050 val_acc= 0.87323 time= 1.94464\n",
      "Epoch: 0174 train_loss= 0.16422 train_acc= 0.94007 val_loss= 0.40441 val_acc= 0.87323 time= 1.95611\n",
      "Epoch: 0175 train_loss= 0.16266 train_acc= 0.94374 val_loss= 0.40754 val_acc= 0.87218 time= 1.90561\n",
      "Epoch: 0176 train_loss= 0.17241 train_acc= 0.93461 val_loss= 0.40621 val_acc= 0.87428 time= 1.91524\n",
      "Epoch: 0177 train_loss= 0.17271 train_acc= 0.93565 val_loss= 0.40181 val_acc= 0.87114 time= 1.93894\n",
      "Epoch: 0178 train_loss= 0.15956 train_acc= 0.94339 val_loss= 0.40058 val_acc= 0.87009 time= 1.95077\n",
      "Epoch: 0179 train_loss= 0.16631 train_acc= 0.93781 val_loss= 0.40144 val_acc= 0.87009 time= 1.93083\n",
      "Epoch: 0180 train_loss= 0.16795 train_acc= 0.93693 val_loss= 0.40210 val_acc= 0.86956 time= 1.94164\n",
      "Epoch: 0181 train_loss= 0.15968 train_acc= 0.94310 val_loss= 0.40253 val_acc= 0.86747 time= 1.95241\n",
      "Epoch: 0182 train_loss= 0.15883 train_acc= 0.94543 val_loss= 0.40264 val_acc= 0.86956 time= 1.91802\n",
      "Epoch: 0183 train_loss= 0.15793 train_acc= 0.94298 val_loss= 0.40253 val_acc= 0.87061 time= 1.95799\n",
      "Epoch: 0184 train_loss= 0.16504 train_acc= 0.93996 val_loss= 0.40550 val_acc= 0.87323 time= 1.95535\n",
      "Epoch: 0185 train_loss= 0.15676 train_acc= 0.94199 val_loss= 0.41028 val_acc= 0.87271 time= 1.92110\n",
      "Epoch: 0186 train_loss= 0.16272 train_acc= 0.93996 val_loss= 0.40997 val_acc= 0.87166 time= 1.92182\n",
      "Epoch: 0187 train_loss= 0.16224 train_acc= 0.93850 val_loss= 0.40606 val_acc= 0.87271 time= 1.93520\n",
      "Epoch: 0188 train_loss= 0.15661 train_acc= 0.94415 val_loss= 0.40342 val_acc= 0.87376 time= 1.90787\n",
      "Epoch: 0189 train_loss= 0.15703 train_acc= 0.94345 val_loss= 0.40555 val_acc= 0.86590 time= 1.91699\n",
      "Epoch: 0190 train_loss= 0.16191 train_acc= 0.94135 val_loss= 0.40699 val_acc= 0.86590 time= 1.92714\n",
      "Epoch: 0191 train_loss= 0.16393 train_acc= 0.93897 val_loss= 0.40543 val_acc= 0.86747 time= 1.97110\n",
      "Epoch: 0192 train_loss= 0.15661 train_acc= 0.94205 val_loss= 0.40486 val_acc= 0.87009 time= 1.89068\n",
      "Epoch: 0193 train_loss= 0.15583 train_acc= 0.94368 val_loss= 0.40925 val_acc= 0.87218 time= 1.91728\n",
      "Epoch: 0194 train_loss= 0.15284 train_acc= 0.94357 val_loss= 0.41528 val_acc= 0.87218 time= 1.89386\n",
      "Epoch: 0195 train_loss= 0.15167 train_acc= 0.94531 val_loss= 0.41784 val_acc= 0.87061 time= 1.96519\n",
      "Epoch: 0196 train_loss= 0.16152 train_acc= 0.93914 val_loss= 0.41448 val_acc= 0.87323 time= 1.94819\n",
      "Epoch: 0197 train_loss= 0.16254 train_acc= 0.93804 val_loss= 0.40850 val_acc= 0.87218 time= 1.89377\n",
      "Epoch: 0198 train_loss= 0.15353 train_acc= 0.94386 val_loss= 0.40969 val_acc= 0.87009 time= 1.93543\n",
      "Epoch: 0199 train_loss= 0.16496 train_acc= 0.93909 val_loss= 0.41092 val_acc= 0.87009 time= 1.92829\n",
      "Epoch: 0200 train_loss= 0.15633 train_acc= 0.94252 val_loss= 0.41100 val_acc= 0.87271 time= 1.95554\n",
      "Epoch: 0201 train_loss= 0.15431 train_acc= 0.94316 val_loss= 0.41331 val_acc= 0.87218 time= 1.95199\n",
      "Epoch: 0202 train_loss= 0.15170 train_acc= 0.94421 val_loss= 0.41588 val_acc= 0.87061 time= 1.93208\n",
      "Epoch: 0203 train_loss= 0.15571 train_acc= 0.94316 val_loss= 0.41410 val_acc= 0.87166 time= 1.93285\n",
      "Epoch: 0204 train_loss= 0.14729 train_acc= 0.94479 val_loss= 0.41242 val_acc= 0.87061 time= 1.93482\n",
      "Epoch: 0205 train_loss= 0.14868 train_acc= 0.94601 val_loss= 0.41299 val_acc= 0.86956 time= 1.91715\n",
      "Epoch: 0206 train_loss= 0.14441 train_acc= 0.95090 val_loss= 0.41476 val_acc= 0.86799 time= 1.87830\n",
      "Epoch: 0207 train_loss= 0.14529 train_acc= 0.94770 val_loss= 0.41611 val_acc= 0.86799 time= 1.87050\n",
      "Epoch: 0208 train_loss= 0.15032 train_acc= 0.94543 val_loss= 0.41695 val_acc= 0.86799 time= 1.88049\n",
      "Epoch: 0209 train_loss= 0.15687 train_acc= 0.93967 val_loss= 0.41728 val_acc= 0.86904 time= 1.91630\n",
      "Epoch: 0210 train_loss= 0.15258 train_acc= 0.94653 val_loss= 0.42044 val_acc= 0.87218 time= 1.90983\n",
      "Epoch: 0211 train_loss= 0.15589 train_acc= 0.94066 val_loss= 0.42255 val_acc= 0.87271 time= 1.90205\n",
      "Epoch: 0212 train_loss= 0.14875 train_acc= 0.94589 val_loss= 0.42416 val_acc= 0.87061 time= 1.88152\n",
      "Epoch: 0213 train_loss= 0.15200 train_acc= 0.94572 val_loss= 0.42300 val_acc= 0.87009 time= 1.90207\n",
      "Epoch: 0214 train_loss= 0.14616 train_acc= 0.94758 val_loss= 0.42194 val_acc= 0.86852 time= 1.89598\n",
      "Epoch: 0215 train_loss= 0.15002 train_acc= 0.94397 val_loss= 0.42326 val_acc= 0.86904 time= 1.87207\n",
      "Epoch: 0216 train_loss= 0.15067 train_acc= 0.94473 val_loss= 0.42397 val_acc= 0.86852 time= 1.91065\n",
      "Epoch: 0217 train_loss= 0.15837 train_acc= 0.93903 val_loss= 0.42208 val_acc= 0.87218 time= 1.90420\n",
      "Epoch: 0218 train_loss= 0.14374 train_acc= 0.95014 val_loss= 0.42136 val_acc= 0.87061 time= 1.92052\n",
      "Epoch: 0219 train_loss= 0.14515 train_acc= 0.94775 val_loss= 0.42304 val_acc= 0.86799 time= 1.89198\n",
      "Epoch: 0220 train_loss= 0.15027 train_acc= 0.94531 val_loss= 0.42339 val_acc= 0.86799 time= 1.89708\n",
      "Epoch: 0221 train_loss= 0.14395 train_acc= 0.94863 val_loss= 0.42251 val_acc= 0.86799 time= 1.90754\n",
      "Epoch: 0222 train_loss= 0.14775 train_acc= 0.94537 val_loss= 0.42086 val_acc= 0.87009 time= 1.88452\n",
      "Epoch: 0223 train_loss= 0.14490 train_acc= 0.94944 val_loss= 0.42074 val_acc= 0.87061 time= 1.91005\n",
      "Epoch: 0224 train_loss= 0.14295 train_acc= 0.94874 val_loss= 0.42342 val_acc= 0.87061 time= 1.90277\n",
      "Epoch: 0225 train_loss= 0.14530 train_acc= 0.94624 val_loss= 0.42422 val_acc= 0.87166 time= 1.91061\n",
      "Epoch: 0226 train_loss= 0.14443 train_acc= 0.94775 val_loss= 0.42310 val_acc= 0.87114 time= 1.90453\n",
      "Epoch: 0227 train_loss= 0.14458 train_acc= 0.94752 val_loss= 0.42144 val_acc= 0.87114 time= 1.89769\n",
      "Epoch: 0228 train_loss= 0.14003 train_acc= 0.95171 val_loss= 0.42134 val_acc= 0.87166 time= 1.90578\n",
      "Epoch: 0229 train_loss= 0.14086 train_acc= 0.95008 val_loss= 0.42211 val_acc= 0.87114 time= 1.93214\n",
      "Epoch: 0230 train_loss= 0.13893 train_acc= 0.95060 val_loss= 0.42322 val_acc= 0.87114 time= 1.90819\n",
      "Epoch: 0231 train_loss= 0.14276 train_acc= 0.94700 val_loss= 0.42427 val_acc= 0.87009 time= 1.88737\n",
      "Epoch: 0232 train_loss= 0.13727 train_acc= 0.95247 val_loss= 0.42674 val_acc= 0.87114 time= 1.89692\n",
      "Epoch: 0233 train_loss= 0.14511 train_acc= 0.94665 val_loss= 0.42745 val_acc= 0.87009 time= 1.89826\n",
      "Epoch: 0234 train_loss= 0.14397 train_acc= 0.94892 val_loss= 0.42639 val_acc= 0.87114 time= 1.91745\n",
      "Epoch: 0235 train_loss= 0.13875 train_acc= 0.94909 val_loss= 0.42499 val_acc= 0.87114 time= 1.93387\n",
      "Epoch: 0236 train_loss= 0.13862 train_acc= 0.95090 val_loss= 0.42376 val_acc= 0.87009 time= 1.91832\n",
      "Epoch: 0237 train_loss= 0.14171 train_acc= 0.94868 val_loss= 0.42317 val_acc= 0.86956 time= 1.92816\n",
      "Epoch: 0238 train_loss= 0.13868 train_acc= 0.95235 val_loss= 0.42407 val_acc= 0.87061 time= 1.93918\n",
      "Epoch: 0239 train_loss= 0.13440 train_acc= 0.95282 val_loss= 0.42499 val_acc= 0.87061 time= 1.88640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0240 train_loss= 0.13162 train_acc= 0.95532 val_loss= 0.42552 val_acc= 0.86904 time= 1.90882\n",
      "Epoch: 0241 train_loss= 0.14704 train_acc= 0.94647 val_loss= 0.42617 val_acc= 0.87114 time= 1.88619\n",
      "Epoch: 0242 train_loss= 0.13535 train_acc= 0.94962 val_loss= 0.43200 val_acc= 0.87271 time= 1.89479\n",
      "Epoch: 0243 train_loss= 0.13780 train_acc= 0.94956 val_loss= 0.43894 val_acc= 0.87218 time= 1.90491\n",
      "Epoch: 0244 train_loss= 0.14061 train_acc= 0.94758 val_loss= 0.44002 val_acc= 0.87166 time= 1.88283\n",
      "Epoch: 0245 train_loss= 0.13563 train_acc= 0.95154 val_loss= 0.43771 val_acc= 0.87428 time= 1.88901\n",
      "Epoch: 0246 train_loss= 0.14772 train_acc= 0.94578 val_loss= 0.43147 val_acc= 0.87271 time= 1.92552\n",
      "Epoch: 0247 train_loss= 0.13523 train_acc= 0.95026 val_loss= 0.43137 val_acc= 0.87218 time= 1.90354\n",
      "Epoch: 0248 train_loss= 0.14376 train_acc= 0.94723 val_loss= 0.43442 val_acc= 0.87166 time= 1.91507\n",
      "Epoch: 0249 train_loss= 0.13773 train_acc= 0.95218 val_loss= 0.43560 val_acc= 0.87271 time= 1.91102\n",
      "Epoch: 0250 train_loss= 0.13659 train_acc= 0.94915 val_loss= 0.43550 val_acc= 0.87271 time= 1.88747\n",
      "Epoch: 0251 train_loss= 0.13714 train_acc= 0.95124 val_loss= 0.43478 val_acc= 0.87585 time= 1.90159\n",
      "Epoch: 0252 train_loss= 0.13002 train_acc= 0.95346 val_loss= 0.43444 val_acc= 0.87428 time= 1.89177\n",
      "Epoch: 0253 train_loss= 0.13360 train_acc= 0.95282 val_loss= 0.43569 val_acc= 0.87323 time= 1.89923\n",
      "Epoch: 0254 train_loss= 0.13116 train_acc= 0.95491 val_loss= 0.43965 val_acc= 0.87376 time= 1.88998\n",
      "Epoch: 0255 train_loss= 0.13640 train_acc= 0.94956 val_loss= 0.44080 val_acc= 0.87323 time= 1.91447\n",
      "Epoch: 0256 train_loss= 0.12855 train_acc= 0.95596 val_loss= 0.44240 val_acc= 0.87376 time= 1.89084\n",
      "Epoch: 0257 train_loss= 0.13307 train_acc= 0.95136 val_loss= 0.44118 val_acc= 0.87376 time= 1.88385\n",
      "Epoch: 0258 train_loss= 0.13819 train_acc= 0.94543 val_loss= 0.43664 val_acc= 0.87376 time= 1.89235\n",
      "Epoch: 0259 train_loss= 0.13035 train_acc= 0.95578 val_loss= 0.43525 val_acc= 0.87061 time= 1.93137\n",
      "Epoch: 0260 train_loss= 0.12945 train_acc= 0.95363 val_loss= 0.43581 val_acc= 0.86904 time= 1.91318\n",
      "Epoch: 0261 train_loss= 0.12944 train_acc= 0.95497 val_loss= 0.43712 val_acc= 0.86747 time= 1.92128\n",
      "Epoch: 0262 train_loss= 0.13633 train_acc= 0.94932 val_loss= 0.43583 val_acc= 0.86852 time= 1.88170\n",
      "Epoch: 0263 train_loss= 0.12971 train_acc= 0.95456 val_loss= 0.43430 val_acc= 0.87114 time= 1.90508\n",
      "Epoch: 0264 train_loss= 0.13483 train_acc= 0.95043 val_loss= 0.43464 val_acc= 0.87376 time= 1.91700\n",
      "Epoch: 0265 train_loss= 0.13386 train_acc= 0.95130 val_loss= 0.43644 val_acc= 0.87533 time= 1.89353\n",
      "Epoch: 0266 train_loss= 0.13057 train_acc= 0.95497 val_loss= 0.43776 val_acc= 0.87533 time= 1.89592\n",
      "Epoch: 0267 train_loss= 0.13264 train_acc= 0.95299 val_loss= 0.44023 val_acc= 0.87585 time= 1.89781\n",
      "Epoch: 0268 train_loss= 0.12714 train_acc= 0.95567 val_loss= 0.44154 val_acc= 0.87585 time= 1.91585\n",
      "Epoch: 0269 train_loss= 0.13490 train_acc= 0.95334 val_loss= 0.43983 val_acc= 0.87637 time= 1.92568\n",
      "Epoch: 0270 train_loss= 0.13277 train_acc= 0.95206 val_loss= 0.43757 val_acc= 0.87480 time= 1.92128\n",
      "Epoch: 0271 train_loss= 0.12891 train_acc= 0.95206 val_loss= 0.43819 val_acc= 0.87323 time= 1.93184\n",
      "Epoch: 0272 train_loss= 0.13316 train_acc= 0.95252 val_loss= 0.43935 val_acc= 0.87166 time= 1.91228\n",
      "Epoch: 0273 train_loss= 0.13372 train_acc= 0.95206 val_loss= 0.43939 val_acc= 0.87218 time= 1.90358\n",
      "Epoch: 0274 train_loss= 0.13232 train_acc= 0.95072 val_loss= 0.43921 val_acc= 0.87323 time= 1.91045\n",
      "Epoch: 0275 train_loss= 0.13009 train_acc= 0.95218 val_loss= 0.44200 val_acc= 0.87218 time= 1.88858\n",
      "Epoch: 0276 train_loss= 0.13212 train_acc= 0.95136 val_loss= 0.44481 val_acc= 0.87323 time= 1.89667\n",
      "Epoch: 0277 train_loss= 0.12627 train_acc= 0.95572 val_loss= 0.44792 val_acc= 0.87428 time= 1.87712\n",
      "Epoch: 0278 train_loss= 0.12709 train_acc= 0.95596 val_loss= 0.44746 val_acc= 0.87271 time= 1.89994\n",
      "Epoch: 0279 train_loss= 0.12841 train_acc= 0.95549 val_loss= 0.44517 val_acc= 0.87480 time= 1.89189\n",
      "Epoch: 0280 train_loss= 0.12169 train_acc= 0.95834 val_loss= 0.44293 val_acc= 0.87637 time= 1.89966\n",
      "Epoch: 0281 train_loss= 0.11980 train_acc= 0.95927 val_loss= 0.44290 val_acc= 0.87637 time= 1.89220\n",
      "Epoch: 0282 train_loss= 0.12632 train_acc= 0.95514 val_loss= 0.44419 val_acc= 0.87637 time= 1.91654\n",
      "Epoch: 0283 train_loss= 0.12642 train_acc= 0.95427 val_loss= 0.44497 val_acc= 0.87376 time= 1.89956\n",
      "Epoch: 0284 train_loss= 0.12602 train_acc= 0.95543 val_loss= 0.44542 val_acc= 0.87376 time= 1.89523\n",
      "Epoch: 0285 train_loss= 0.12583 train_acc= 0.95555 val_loss= 0.44695 val_acc= 0.86904 time= 1.90727\n",
      "Epoch: 0286 train_loss= 0.14636 train_acc= 0.94519 val_loss= 0.44576 val_acc= 0.87061 time= 1.88528\n",
      "Epoch: 0287 train_loss= 0.12966 train_acc= 0.95299 val_loss= 0.44617 val_acc= 0.87271 time= 1.88057\n",
      "Epoch: 0288 train_loss= 0.12415 train_acc= 0.95468 val_loss= 0.44800 val_acc= 0.87428 time= 1.91856\n",
      "Epoch: 0289 train_loss= 0.12381 train_acc= 0.95642 val_loss= 0.45266 val_acc= 0.87428 time= 1.89421\n",
      "Epoch: 0290 train_loss= 0.13334 train_acc= 0.94950 val_loss= 0.45317 val_acc= 0.87428 time= 1.90346\n",
      "Epoch: 0291 train_loss= 0.12011 train_acc= 0.95625 val_loss= 0.45119 val_acc= 0.87742 time= 1.90047\n",
      "Epoch: 0292 train_loss= 0.12198 train_acc= 0.95700 val_loss= 0.44867 val_acc= 0.87742 time= 1.93566\n",
      "Epoch: 0293 train_loss= 0.12508 train_acc= 0.95799 val_loss= 0.44699 val_acc= 0.87533 time= 1.92866\n",
      "Epoch: 0294 train_loss= 0.12008 train_acc= 0.95927 val_loss= 0.44645 val_acc= 0.87428 time= 1.94132\n",
      "Epoch: 0295 train_loss= 0.12011 train_acc= 0.95863 val_loss= 0.44738 val_acc= 0.87533 time= 1.92085\n",
      "Epoch: 0296 train_loss= 0.12031 train_acc= 0.95799 val_loss= 0.44785 val_acc= 0.87533 time= 1.89655\n",
      "Epoch: 0297 train_loss= 0.14144 train_acc= 0.94572 val_loss= 0.44635 val_acc= 0.87271 time= 1.88562\n",
      "Epoch: 0298 train_loss= 0.12499 train_acc= 0.95648 val_loss= 0.44709 val_acc= 0.87585 time= 1.91102\n",
      "Epoch: 0299 train_loss= 0.11828 train_acc= 0.95974 val_loss= 0.45143 val_acc= 0.87533 time= 1.90584\n",
      "Epoch: 0300 train_loss= 0.11835 train_acc= 0.95764 val_loss= 0.45651 val_acc= 0.87533 time= 1.89891\n",
      "Epoch: 0301 train_loss= 0.12674 train_acc= 0.95491 val_loss= 0.45812 val_acc= 0.87480 time= 1.90779\n",
      "Epoch: 0302 train_loss= 0.12433 train_acc= 0.95322 val_loss= 0.45639 val_acc= 0.87428 time= 1.90215\n",
      "Epoch: 0303 train_loss= 0.11871 train_acc= 0.95782 val_loss= 0.45529 val_acc= 0.87480 time= 1.90905\n",
      "Epoch: 0304 train_loss= 0.11986 train_acc= 0.95776 val_loss= 0.45539 val_acc= 0.87637 time= 1.90261\n",
      "Epoch: 0305 train_loss= 0.11908 train_acc= 0.95584 val_loss= 0.45463 val_acc= 0.87690 time= 1.92829\n",
      "Epoch: 0306 train_loss= 0.12028 train_acc= 0.95712 val_loss= 0.45575 val_acc= 0.87742 time= 1.92456\n",
      "Epoch: 0307 train_loss= 0.11813 train_acc= 0.95968 val_loss= 0.45661 val_acc= 0.87690 time= 1.90256\n",
      "Epoch: 0308 train_loss= 0.12436 train_acc= 0.95538 val_loss= 0.45559 val_acc= 0.87690 time= 1.91215\n",
      "Epoch: 0309 train_loss= 0.12318 train_acc= 0.95677 val_loss= 0.45520 val_acc= 0.87480 time= 1.90690\n",
      "Epoch: 0310 train_loss= 0.11611 train_acc= 0.96009 val_loss= 0.45783 val_acc= 0.87376 time= 1.89964\n",
      "Epoch: 0311 train_loss= 0.12082 train_acc= 0.95625 val_loss= 0.46346 val_acc= 0.87218 time= 1.89345\n",
      "Epoch: 0312 train_loss= 0.12234 train_acc= 0.95456 val_loss= 0.46772 val_acc= 0.87166 time= 1.90080\n",
      "Epoch: 0313 train_loss= 0.12953 train_acc= 0.95171 val_loss= 0.46541 val_acc= 0.87323 time= 1.89416\n",
      "Epoch: 0314 train_loss= 0.12158 train_acc= 0.95561 val_loss= 0.46169 val_acc= 0.87271 time= 1.93318\n",
      "Epoch: 0315 train_loss= 0.11330 train_acc= 0.96084 val_loss= 0.45978 val_acc= 0.87376 time= 1.90891\n",
      "Epoch: 0316 train_loss= 0.11768 train_acc= 0.95857 val_loss= 0.45897 val_acc= 0.87690 time= 1.89282\n",
      "Epoch: 0317 train_loss= 0.12283 train_acc= 0.95671 val_loss= 0.45928 val_acc= 0.87637 time= 1.89997\n",
      "Epoch: 0318 train_loss= 0.11775 train_acc= 0.95892 val_loss= 0.45959 val_acc= 0.87585 time= 1.89051\n",
      "Epoch: 0319 train_loss= 0.11733 train_acc= 0.95770 val_loss= 0.46013 val_acc= 0.87585 time= 1.89979\n",
      "Epoch: 0320 train_loss= 0.12223 train_acc= 0.95590 val_loss= 0.46341 val_acc= 0.87533 time= 1.90979\n",
      "Epoch: 0321 train_loss= 0.11227 train_acc= 0.96119 val_loss= 0.46769 val_acc= 0.87533 time= 1.91840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0322 train_loss= 0.11647 train_acc= 0.95910 val_loss= 0.47067 val_acc= 0.87533 time= 1.93375\n",
      "Epoch: 0323 train_loss= 0.13609 train_acc= 0.94642 val_loss= 0.46595 val_acc= 0.87480 time= 1.89263\n",
      "Epoch: 0324 train_loss= 0.11105 train_acc= 0.96038 val_loss= 0.46225 val_acc= 0.87690 time= 1.94665\n",
      "Epoch: 0325 train_loss= 0.11060 train_acc= 0.96224 val_loss= 0.46091 val_acc= 0.87585 time= 1.89785\n",
      "Epoch: 0326 train_loss= 0.11488 train_acc= 0.95956 val_loss= 0.46097 val_acc= 0.87428 time= 1.89188\n",
      "Epoch: 0327 train_loss= 0.11131 train_acc= 0.96172 val_loss= 0.46135 val_acc= 0.87271 time= 1.89742\n",
      "Epoch: 0328 train_loss= 0.11801 train_acc= 0.95881 val_loss= 0.46137 val_acc= 0.87323 time= 1.90589\n",
      "Epoch: 0329 train_loss= 0.11512 train_acc= 0.96166 val_loss= 0.46300 val_acc= 0.87376 time= 1.89497\n",
      "Epoch: 0330 train_loss= 0.11469 train_acc= 0.95793 val_loss= 0.46584 val_acc= 0.87376 time= 1.93684\n",
      "Epoch: 0331 train_loss= 0.10734 train_acc= 0.96422 val_loss= 0.46922 val_acc= 0.87480 time= 1.90143\n",
      "Epoch: 0332 train_loss= 0.10994 train_acc= 0.96096 val_loss= 0.47098 val_acc= 0.87533 time= 1.92401\n",
      "Epoch: 0333 train_loss= 0.11394 train_acc= 0.96026 val_loss= 0.47116 val_acc= 0.87480 time= 1.90554\n",
      "Epoch: 0334 train_loss= 0.11236 train_acc= 0.96096 val_loss= 0.46970 val_acc= 0.87376 time= 1.89680\n",
      "Epoch: 0335 train_loss= 0.11404 train_acc= 0.96172 val_loss= 0.46865 val_acc= 0.87533 time= 1.88733\n",
      "Epoch: 0336 train_loss= 0.11525 train_acc= 0.95834 val_loss= 0.46876 val_acc= 0.87376 time= 1.91066\n",
      "Epoch: 0337 train_loss= 0.11296 train_acc= 0.96102 val_loss= 0.46953 val_acc= 0.87480 time= 1.90316\n",
      "Epoch: 0338 train_loss= 0.11865 train_acc= 0.95921 val_loss= 0.47008 val_acc= 0.87271 time= 1.96060\n",
      "Epoch: 0339 train_loss= 0.12021 train_acc= 0.95741 val_loss= 0.47101 val_acc= 0.87271 time= 1.90804\n",
      "Epoch: 0340 train_loss= 0.10869 train_acc= 0.96131 val_loss= 0.47378 val_acc= 0.87376 time= 1.86992\n",
      "Epoch: 0341 train_loss= 0.11555 train_acc= 0.95904 val_loss= 0.47836 val_acc= 0.87637 time= 1.87684\n",
      "Epoch: 0342 train_loss= 0.11295 train_acc= 0.95939 val_loss= 0.48127 val_acc= 0.87533 time= 1.94413\n",
      "Epoch: 0343 train_loss= 0.12833 train_acc= 0.94915 val_loss= 0.47875 val_acc= 0.87428 time= 1.91107\n",
      "Epoch: 0344 train_loss= 0.10846 train_acc= 0.96230 val_loss= 0.47671 val_acc= 0.87533 time= 1.87445\n",
      "Epoch: 0345 train_loss= 0.11185 train_acc= 0.96084 val_loss= 0.47619 val_acc= 0.87637 time= 1.89790\n",
      "Epoch: 0346 train_loss= 0.10956 train_acc= 0.96189 val_loss= 0.47673 val_acc= 0.87428 time= 1.90613\n",
      "Epoch: 0347 train_loss= 0.11295 train_acc= 0.96113 val_loss= 0.47691 val_acc= 0.87585 time= 1.90384\n",
      "Epoch: 0348 train_loss= 0.12276 train_acc= 0.95724 val_loss= 0.47652 val_acc= 0.87376 time= 1.92830\n",
      "Epoch: 0349 train_loss= 0.11174 train_acc= 0.96096 val_loss= 0.47920 val_acc= 0.87323 time= 1.90001\n",
      "Epoch: 0350 train_loss= 0.10806 train_acc= 0.96364 val_loss= 0.48424 val_acc= 0.87323 time= 1.89658\n",
      "Epoch: 0351 train_loss= 0.10984 train_acc= 0.96160 val_loss= 0.48801 val_acc= 0.87480 time= 1.90449\n",
      "Epoch: 0352 train_loss= 0.11826 train_acc= 0.95747 val_loss= 0.48737 val_acc= 0.87323 time= 1.88233\n",
      "Epoch: 0353 train_loss= 0.12073 train_acc= 0.95636 val_loss= 0.48215 val_acc= 0.87218 time= 1.90509\n",
      "Epoch: 0354 train_loss= 0.11024 train_acc= 0.96108 val_loss= 0.47861 val_acc= 0.87376 time= 1.91616\n",
      "Epoch: 0355 train_loss= 0.10835 train_acc= 0.96410 val_loss= 0.47854 val_acc= 0.87585 time= 1.94224\n",
      "Epoch: 0356 train_loss= 0.10988 train_acc= 0.96166 val_loss= 0.48123 val_acc= 0.87323 time= 1.91805\n",
      "Epoch: 0357 train_loss= 0.11510 train_acc= 0.95945 val_loss= 0.48313 val_acc= 0.87271 time= 1.88118\n",
      "Epoch: 0358 train_loss= 0.10879 train_acc= 0.96247 val_loss= 0.48356 val_acc= 0.87166 time= 1.90474\n",
      "Epoch: 0359 train_loss= 0.11146 train_acc= 0.96090 val_loss= 0.48131 val_acc= 0.87323 time= 1.92515\n",
      "Epoch: 0360 train_loss= 0.11624 train_acc= 0.95846 val_loss= 0.47824 val_acc= 0.87637 time= 1.91548\n",
      "Epoch: 0361 train_loss= 0.10953 train_acc= 0.96247 val_loss= 0.47729 val_acc= 0.87480 time= 1.92077\n",
      "Epoch: 0362 train_loss= 0.10482 train_acc= 0.96271 val_loss= 0.47904 val_acc= 0.87376 time= 1.91505\n",
      "Epoch: 0363 train_loss= 0.10499 train_acc= 0.96329 val_loss= 0.48327 val_acc= 0.87271 time= 1.89356\n",
      "Epoch: 0364 train_loss= 0.11433 train_acc= 0.95898 val_loss= 0.48625 val_acc= 0.87637 time= 1.93102\n",
      "Epoch: 0365 train_loss= 0.11528 train_acc= 0.95916 val_loss= 0.48626 val_acc= 0.87480 time= 1.89555\n",
      "Epoch: 0366 train_loss= 0.11326 train_acc= 0.95793 val_loss= 0.48417 val_acc= 0.87376 time= 1.90484\n",
      "Epoch: 0367 train_loss= 0.10974 train_acc= 0.96137 val_loss= 0.48337 val_acc= 0.87690 time= 1.89919\n",
      "Epoch: 0368 train_loss= 0.09885 train_acc= 0.96654 val_loss= 0.48504 val_acc= 0.87428 time= 1.89295\n",
      "Epoch: 0369 train_loss= 0.11089 train_acc= 0.96038 val_loss= 0.48712 val_acc= 0.87480 time= 1.90089\n",
      "Epoch: 0370 train_loss= 0.11230 train_acc= 0.96073 val_loss= 0.48631 val_acc= 0.87585 time= 1.89129\n",
      "Epoch: 0371 train_loss= 0.11413 train_acc= 0.95898 val_loss= 0.48420 val_acc= 0.87480 time= 1.92956\n",
      "Epoch: 0372 train_loss= 0.10750 train_acc= 0.96172 val_loss= 0.48223 val_acc= 0.87585 time= 1.94019\n",
      "Epoch: 0373 train_loss= 0.10514 train_acc= 0.96445 val_loss= 0.48220 val_acc= 0.87690 time= 1.91918\n",
      "Epoch: 0374 train_loss= 0.10649 train_acc= 0.96177 val_loss= 0.48292 val_acc= 0.87323 time= 1.89938\n",
      "Epoch: 0375 train_loss= 0.10263 train_acc= 0.96503 val_loss= 0.48430 val_acc= 0.87166 time= 1.89172\n",
      "Epoch: 0376 train_loss= 0.11360 train_acc= 0.95276 val_loss= 0.48528 val_acc= 0.87166 time= 1.90054\n",
      "Epoch: 0377 train_loss= 0.10918 train_acc= 0.95945 val_loss= 0.48527 val_acc= 0.87637 time= 1.92436\n",
      "Epoch: 0378 train_loss= 0.10508 train_acc= 0.96416 val_loss= 0.48697 val_acc= 0.87637 time= 1.91446\n",
      "Epoch: 0379 train_loss= 0.10246 train_acc= 0.96346 val_loss= 0.49182 val_acc= 0.87533 time= 1.94209\n",
      "Epoch: 0380 train_loss= 0.10749 train_acc= 0.96433 val_loss= 0.49692 val_acc= 0.87585 time= 1.94978\n",
      "Epoch: 0381 train_loss= 0.10405 train_acc= 0.96433 val_loss= 0.50057 val_acc= 0.87428 time= 1.91466\n",
      "Epoch: 0382 train_loss= 0.11378 train_acc= 0.96061 val_loss= 0.50042 val_acc= 0.87376 time= 1.92793\n",
      "Epoch: 0383 train_loss= 0.12248 train_acc= 0.95677 val_loss= 0.49560 val_acc= 0.87637 time= 1.92930\n",
      "Epoch: 0384 train_loss= 0.10872 train_acc= 0.96247 val_loss= 0.49188 val_acc= 0.87690 time= 1.92746\n",
      "Epoch: 0385 train_loss= 0.10025 train_acc= 0.96654 val_loss= 0.49147 val_acc= 0.87166 time= 1.94115\n",
      "Epoch: 0386 train_loss= 0.10687 train_acc= 0.96224 val_loss= 0.49376 val_acc= 0.87218 time= 1.89660\n",
      "Epoch: 0387 train_loss= 0.10916 train_acc= 0.96154 val_loss= 0.49559 val_acc= 0.87061 time= 1.97226\n",
      "Epoch: 0388 train_loss= 0.10891 train_acc= 0.95980 val_loss= 0.49517 val_acc= 0.87061 time= 1.89408\n",
      "Epoch: 0389 train_loss= 0.09755 train_acc= 0.96596 val_loss= 0.49500 val_acc= 0.87271 time= 1.92506\n",
      "Epoch: 0390 train_loss= 0.09600 train_acc= 0.96730 val_loss= 0.49575 val_acc= 0.87480 time= 1.91207\n",
      "Epoch: 0391 train_loss= 0.10033 train_acc= 0.96631 val_loss= 0.49717 val_acc= 0.87637 time= 1.89675\n",
      "Epoch: 0392 train_loss= 0.10507 train_acc= 0.96276 val_loss= 0.49855 val_acc= 0.87585 time= 1.94411\n",
      "Epoch: 0393 train_loss= 0.09847 train_acc= 0.96858 val_loss= 0.50002 val_acc= 0.87585 time= 1.92792\n",
      "Epoch: 0394 train_loss= 0.10848 train_acc= 0.96166 val_loss= 0.50126 val_acc= 0.87585 time= 1.89392\n",
      "Epoch: 0395 train_loss= 0.10249 train_acc= 0.96375 val_loss= 0.50105 val_acc= 0.87690 time= 1.92780\n",
      "Epoch: 0396 train_loss= 0.10250 train_acc= 0.96393 val_loss= 0.50000 val_acc= 0.87637 time= 1.91401\n",
      "Epoch: 0397 train_loss= 0.10510 train_acc= 0.96154 val_loss= 0.49785 val_acc= 0.87690 time= 1.91276\n",
      "Epoch: 0398 train_loss= 0.09802 train_acc= 0.96742 val_loss= 0.49642 val_acc= 0.87480 time= 1.92542\n",
      "Epoch: 0399 train_loss= 0.10000 train_acc= 0.96521 val_loss= 0.49651 val_acc= 0.87323 time= 1.91030\n",
      "Epoch: 0400 train_loss= 0.09757 train_acc= 0.96771 val_loss= 0.49787 val_acc= 0.87218 time= 1.90951\n",
      "Epoch: 0401 train_loss= 0.10537 train_acc= 0.96358 val_loss= 0.49941 val_acc= 0.87114 time= 1.91176\n",
      "Epoch: 0402 train_loss= 0.11280 train_acc= 0.96177 val_loss= 0.49990 val_acc= 0.87061 time= 1.92521\n",
      "Epoch: 0403 train_loss= 0.11619 train_acc= 0.95578 val_loss= 0.50077 val_acc= 0.87218 time= 1.93235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0404 train_loss= 0.10390 train_acc= 0.96369 val_loss= 0.50333 val_acc= 0.87428 time= 1.89666\n",
      "Epoch: 0405 train_loss= 0.10011 train_acc= 0.96684 val_loss= 0.50599 val_acc= 0.87480 time= 1.94390\n",
      "Epoch: 0406 train_loss= 0.10741 train_acc= 0.96090 val_loss= 0.50859 val_acc= 0.87533 time= 1.91299\n",
      "Epoch: 0407 train_loss= 0.10510 train_acc= 0.96335 val_loss= 0.50927 val_acc= 0.87428 time= 1.87805\n",
      "Epoch: 0408 train_loss= 0.09779 train_acc= 0.96707 val_loss= 0.50827 val_acc= 0.87533 time= 1.87918\n",
      "Epoch: 0409 train_loss= 0.10304 train_acc= 0.96422 val_loss= 0.50674 val_acc= 0.87271 time= 1.87836\n",
      "Epoch: 0410 train_loss= 0.09800 train_acc= 0.96812 val_loss= 0.50615 val_acc= 0.87166 time= 1.89664\n",
      "Epoch: 0411 train_loss= 0.10159 train_acc= 0.96497 val_loss= 0.50633 val_acc= 0.87166 time= 1.94566\n",
      "Epoch: 0412 train_loss= 0.10276 train_acc= 0.96486 val_loss= 0.50730 val_acc= 0.87114 time= 1.92995\n",
      "Epoch: 0413 train_loss= 0.10584 train_acc= 0.96282 val_loss= 0.50811 val_acc= 0.87061 time= 1.92939\n",
      "Epoch: 0414 train_loss= 0.09734 train_acc= 0.96730 val_loss= 0.50869 val_acc= 0.87114 time= 1.91435\n",
      "Epoch: 0415 train_loss= 0.10327 train_acc= 0.96364 val_loss= 0.50806 val_acc= 0.87323 time= 1.89364\n",
      "Epoch: 0416 train_loss= 0.09599 train_acc= 0.96707 val_loss= 0.50776 val_acc= 0.87428 time= 1.92586\n",
      "Epoch: 0417 train_loss= 0.10080 train_acc= 0.96573 val_loss= 0.50906 val_acc= 0.87428 time= 1.91412\n",
      "Epoch: 0418 train_loss= 0.09398 train_acc= 0.96753 val_loss= 0.51137 val_acc= 0.87533 time= 1.94056\n",
      "Epoch: 0419 train_loss= 0.11499 train_acc= 0.95770 val_loss= 0.51044 val_acc= 0.87637 time= 1.91373\n",
      "Epoch: 0420 train_loss= 0.10299 train_acc= 0.96428 val_loss= 0.51045 val_acc= 0.87428 time= 1.89673\n",
      "Epoch: 0421 train_loss= 0.09372 train_acc= 0.96806 val_loss= 0.51144 val_acc= 0.87428 time= 1.89569\n",
      "Epoch: 0422 train_loss= 0.09747 train_acc= 0.96637 val_loss= 0.51261 val_acc= 0.87480 time= 1.91909\n",
      "Epoch: 0423 train_loss= 0.09766 train_acc= 0.96602 val_loss= 0.51502 val_acc= 0.87376 time= 1.89554\n",
      "Epoch: 0424 train_loss= 0.09903 train_acc= 0.96515 val_loss= 0.51479 val_acc= 0.87166 time= 1.94120\n",
      "Epoch: 0425 train_loss= 0.09595 train_acc= 0.96678 val_loss= 0.51282 val_acc= 0.87376 time= 1.95986\n",
      "Epoch: 0426 train_loss= 0.09895 train_acc= 0.96521 val_loss= 0.50983 val_acc= 0.87271 time= 1.91008\n",
      "Epoch: 0427 train_loss= 0.09275 train_acc= 0.96969 val_loss= 0.50842 val_acc= 0.87428 time= 1.96069\n",
      "Epoch: 0428 train_loss= 0.09269 train_acc= 0.96812 val_loss= 0.50882 val_acc= 0.87218 time= 1.89378\n",
      "Epoch: 0429 train_loss= 0.09600 train_acc= 0.96689 val_loss= 0.50979 val_acc= 0.87114 time= 1.94552\n",
      "Epoch: 0430 train_loss= 0.09335 train_acc= 0.96841 val_loss= 0.51026 val_acc= 0.87061 time= 1.93112\n",
      "Epoch: 0431 train_loss= 0.10115 train_acc= 0.96497 val_loss= 0.50962 val_acc= 0.87218 time= 1.90981\n",
      "Epoch: 0432 train_loss= 0.09907 train_acc= 0.96492 val_loss= 0.50920 val_acc= 0.87376 time= 1.94786\n",
      "Epoch: 0433 train_loss= 0.09391 train_acc= 0.96678 val_loss= 0.51030 val_acc= 0.87480 time= 1.92844\n",
      "Epoch: 0434 train_loss= 0.09462 train_acc= 0.96817 val_loss= 0.51368 val_acc= 0.87428 time= 1.94657\n",
      "Epoch: 0435 train_loss= 0.08783 train_acc= 0.97091 val_loss= 0.51717 val_acc= 0.87323 time= 1.94287\n",
      "Epoch: 0436 train_loss= 0.09701 train_acc= 0.96579 val_loss= 0.51909 val_acc= 0.87114 time= 1.93106\n",
      "Epoch: 0437 train_loss= 0.09723 train_acc= 0.96625 val_loss= 0.51944 val_acc= 0.87114 time= 1.94111\n",
      "Epoch: 0438 train_loss= 0.09343 train_acc= 0.96934 val_loss= 0.51921 val_acc= 0.87271 time= 1.99362\n",
      "Epoch: 0439 train_loss= 0.10998 train_acc= 0.96032 val_loss= 0.51516 val_acc= 0.87533 time= 1.95919\n",
      "Epoch: 0440 train_loss= 0.09344 train_acc= 0.96829 val_loss= 0.51354 val_acc= 0.87585 time= 1.96403\n",
      "Epoch: 0441 train_loss= 0.09509 train_acc= 0.96596 val_loss= 0.51323 val_acc= 0.87585 time= 1.91178\n",
      "Epoch: 0442 train_loss= 0.09838 train_acc= 0.96707 val_loss= 0.51359 val_acc= 0.87585 time= 1.90991\n",
      "Epoch: 0443 train_loss= 0.09444 train_acc= 0.96713 val_loss= 0.51363 val_acc= 0.87585 time= 1.87931\n",
      "Epoch: 0444 train_loss= 0.08721 train_acc= 0.97149 val_loss= 0.51374 val_acc= 0.87585 time= 1.91162\n",
      "Epoch: 0445 train_loss= 0.09246 train_acc= 0.96893 val_loss= 0.51427 val_acc= 0.87585 time= 1.90967\n",
      "Epoch: 0446 train_loss= 0.09349 train_acc= 0.96788 val_loss= 0.51583 val_acc= 0.87585 time= 1.87807\n",
      "Epoch: 0447 train_loss= 0.09291 train_acc= 0.96684 val_loss= 0.51748 val_acc= 0.87637 time= 1.92662\n",
      "Epoch: 0448 train_loss= 0.08470 train_acc= 0.97149 val_loss= 0.51899 val_acc= 0.87742 time= 1.90981\n",
      "Epoch: 0449 train_loss= 0.09392 train_acc= 0.96654 val_loss= 0.51894 val_acc= 0.87637 time= 1.94563\n",
      "Epoch: 0450 train_loss= 0.09795 train_acc= 0.96782 val_loss= 0.51675 val_acc= 0.87585 time= 1.92838\n",
      "Epoch: 0451 train_loss= 0.08970 train_acc= 0.96934 val_loss= 0.51571 val_acc= 0.87585 time= 1.94221\n",
      "Epoch: 0452 train_loss= 0.08760 train_acc= 0.97027 val_loss= 0.51536 val_acc= 0.87480 time= 1.89457\n",
      "Epoch: 0453 train_loss= 0.08818 train_acc= 0.97033 val_loss= 0.51520 val_acc= 0.87480 time= 1.89654\n",
      "Epoch: 0454 train_loss= 0.09073 train_acc= 0.96777 val_loss= 0.51456 val_acc= 0.87480 time= 1.91264\n",
      "Epoch: 0455 train_loss= 0.08823 train_acc= 0.96980 val_loss= 0.51443 val_acc= 0.87480 time= 1.90953\n",
      "Epoch: 0456 train_loss= 0.09331 train_acc= 0.96753 val_loss= 0.51470 val_acc= 0.87585 time= 1.89669\n",
      "Epoch: 0457 train_loss= 0.09243 train_acc= 0.96771 val_loss= 0.51533 val_acc= 0.87533 time= 1.89612\n",
      "Epoch: 0458 train_loss= 0.09063 train_acc= 0.96806 val_loss= 0.51593 val_acc= 0.87428 time= 1.87806\n",
      "Epoch: 0459 train_loss= 0.08819 train_acc= 0.97079 val_loss= 0.51604 val_acc= 0.87428 time= 1.91309\n",
      "Epoch: 0460 train_loss= 0.09296 train_acc= 0.96998 val_loss= 0.51616 val_acc= 0.87480 time= 1.92841\n",
      "Epoch: 0461 train_loss= 0.08701 train_acc= 0.97225 val_loss= 0.51730 val_acc= 0.87376 time= 1.88424\n",
      "Epoch: 0462 train_loss= 0.09251 train_acc= 0.96945 val_loss= 0.51962 val_acc= 0.87323 time= 1.89577\n",
      "Epoch: 0463 train_loss= 0.09137 train_acc= 0.96951 val_loss= 0.52198 val_acc= 0.87480 time= 1.90942\n",
      "Epoch: 0464 train_loss= 0.09438 train_acc= 0.96637 val_loss= 0.52265 val_acc= 0.87480 time= 1.91022\n",
      "Epoch: 0465 train_loss= 0.08829 train_acc= 0.97062 val_loss= 0.52213 val_acc= 0.87323 time= 1.89371\n",
      "Epoch: 0466 train_loss= 0.09089 train_acc= 0.96963 val_loss= 0.52159 val_acc= 0.87376 time= 1.89384\n",
      "Epoch: 0467 train_loss= 0.08845 train_acc= 0.97091 val_loss= 0.52121 val_acc= 0.87323 time= 1.87955\n",
      "Epoch: 0468 train_loss= 0.08806 train_acc= 0.97126 val_loss= 0.52080 val_acc= 0.87637 time= 1.88175\n",
      "Epoch: 0469 train_loss= 0.08738 train_acc= 0.97149 val_loss= 0.52069 val_acc= 0.87637 time= 1.89382\n",
      "Epoch: 0470 train_loss= 0.08506 train_acc= 0.97085 val_loss= 0.52130 val_acc= 0.87637 time= 1.89469\n",
      "Epoch: 0471 train_loss= 0.09124 train_acc= 0.96800 val_loss= 0.52190 val_acc= 0.87585 time= 1.93751\n",
      "Epoch: 0472 train_loss= 0.09105 train_acc= 0.96864 val_loss= 0.52202 val_acc= 0.87690 time= 1.94294\n",
      "Epoch: 0473 train_loss= 0.08312 train_acc= 0.97388 val_loss= 0.52245 val_acc= 0.87637 time= 1.89391\n",
      "Epoch: 0474 train_loss= 0.08857 train_acc= 0.96945 val_loss= 0.52319 val_acc= 0.87795 time= 1.90957\n",
      "Epoch: 0475 train_loss= 0.09174 train_acc= 0.96951 val_loss= 0.52418 val_acc= 0.87690 time= 1.91329\n",
      "Epoch: 0476 train_loss= 0.09079 train_acc= 0.96812 val_loss= 0.52495 val_acc= 0.87690 time= 1.88302\n",
      "Epoch: 0477 train_loss= 0.09103 train_acc= 0.96905 val_loss= 0.52562 val_acc= 0.87376 time= 1.89669\n",
      "Epoch: 0478 train_loss= 0.08182 train_acc= 0.97388 val_loss= 0.52658 val_acc= 0.87323 time= 1.89697\n",
      "Epoch: 0479 train_loss= 0.09166 train_acc= 0.96777 val_loss= 0.52627 val_acc= 0.87376 time= 1.92771\n",
      "Epoch: 0480 train_loss= 0.08383 train_acc= 0.97225 val_loss= 0.52644 val_acc= 0.87428 time= 1.91160\n",
      "Epoch: 0481 train_loss= 0.08951 train_acc= 0.96771 val_loss= 0.52709 val_acc= 0.87480 time= 1.94283\n",
      "Epoch: 0482 train_loss= 0.08621 train_acc= 0.96916 val_loss= 0.52766 val_acc= 0.87585 time= 1.91103\n",
      "Epoch: 0483 train_loss= 0.08274 train_acc= 0.97102 val_loss= 0.52891 val_acc= 0.87533 time= 1.89894\n",
      "Epoch: 0484 train_loss= 0.08916 train_acc= 0.96893 val_loss= 0.53177 val_acc= 0.87480 time= 1.91617\n",
      "Epoch: 0485 train_loss= 0.08708 train_acc= 0.96893 val_loss= 0.53478 val_acc= 0.87533 time= 1.90433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0486 train_loss= 0.09681 train_acc= 0.96969 val_loss= 0.53499 val_acc= 0.87323 time= 1.92353\n",
      "Epoch: 0487 train_loss= 0.10661 train_acc= 0.96044 val_loss= 0.53078 val_acc= 0.87690 time= 1.91089\n",
      "Epoch: 0488 train_loss= 0.08652 train_acc= 0.97102 val_loss= 0.52714 val_acc= 0.87742 time= 1.91284\n",
      "Epoch: 0489 train_loss= 0.08613 train_acc= 0.97073 val_loss= 0.52654 val_acc= 0.87690 time= 1.86595\n",
      "Epoch: 0490 train_loss= 0.08481 train_acc= 0.96928 val_loss= 0.52887 val_acc= 0.87480 time= 1.89659\n",
      "Epoch: 0491 train_loss= 0.08642 train_acc= 0.96998 val_loss= 0.53208 val_acc= 0.87271 time= 1.90971\n",
      "Epoch: 0492 train_loss= 0.08822 train_acc= 0.96899 val_loss= 0.53450 val_acc= 0.87323 time= 1.94240\n",
      "Epoch: 0493 train_loss= 0.08791 train_acc= 0.97091 val_loss= 0.53480 val_acc= 0.87218 time= 1.92659\n",
      "Epoch: 0494 train_loss= 0.08984 train_acc= 0.96870 val_loss= 0.53350 val_acc= 0.87114 time= 1.94123\n",
      "Epoch: 0495 train_loss= 0.08474 train_acc= 0.97079 val_loss= 0.53290 val_acc= 0.87637 time= 1.90989\n",
      "Epoch: 0496 train_loss= 0.08392 train_acc= 0.97265 val_loss= 0.53412 val_acc= 0.87480 time= 1.89379\n",
      "Epoch: 0497 train_loss= 0.08334 train_acc= 0.97219 val_loss= 0.53532 val_acc= 0.87585 time= 1.89381\n",
      "Epoch: 0498 train_loss= 0.07863 train_acc= 0.97329 val_loss= 0.53683 val_acc= 0.87533 time= 1.90996\n",
      "Epoch: 0499 train_loss= 0.09262 train_acc= 0.96806 val_loss= 0.53708 val_acc= 0.87533 time= 1.89674\n",
      "Epoch: 0500 train_loss= 0.08117 train_acc= 0.97161 val_loss= 0.53647 val_acc= 0.87533 time= 1.87817\n"
     ]
    }
   ],
   "source": [
    "#training model - GCN\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    with tf.GradientTape() as tape:\n",
    "        _, loss, acc = model((t_features, ty_train, tm_train_mask, t_support))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    _, val_loss, val_acc = model((t_features, ty_val, tm_val_mask, t_support), training=False)\n",
    "    cost_.append(val_loss)\n",
    "    \n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(loss),\n",
    "          \"train_acc=\", \"{:.5f}\".format(acc), \"val_loss=\", \"{:.5f}\".format(val_loss),\n",
    "          \"val_acc=\", \"{:.5f}\".format(val_acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "    \n",
    "    # if epoch > early_stopping and cost_val[-1] > np.mean(cost_[-(early_stopping+1):-1]):\n",
    "    #     print(\"Early stopping...\")\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f111bb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1632576552006,
     "user": {
      "displayName": "Palak techies",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04966026704557831019"
     },
     "user_tz": -330
    },
    "id": "7f111bb9",
    "outputId": "fbd94a48-fcae-44f4-e151-f2d872b13da2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy= 0.97161 training loss= 0.08117 time taken= 1.89429\n"
     ]
    }
   ],
   "source": [
    "print(\"training accuracy=\", \"{:.5f}\".format(acc), \"training loss=\", \"{:.5f}\".format(loss), \"time taken=\", \"{:.5f}\".format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8122745",
   "metadata": {
    "id": "f8122745"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc272855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
