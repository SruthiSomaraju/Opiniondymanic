{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "970ebf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'R8'\n",
    "learning_rate = 0.02  \n",
    "epochs  = 100  # Number of epochs to train.\n",
    "hidden1 = 200  # Number of units in hidden layer 1.\n",
    "dropout = 0.5  # Dropout rate (1 - keep probability).\n",
    "weight_decay = 0.   # Weight for L2 loss on embedding matrix.\n",
    "early_stopping = 10 # Tolerance for early stopping (# of epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d93f6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e25ea893",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 6606\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "def masked_softmax_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"\n",
    "    Softmax cross-entropy loss with masking.\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"\n",
    "    Accuracy with masking.\n",
    "    \"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    return tf.reduce_mean(accuracy_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11931e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "\n",
    "def load_corpus(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input corpus from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.train.index => the indices of training docs in original doc list.\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open('cleaned_data/' + dataset_str + '/graph/ind.' + dataset_str + '.' + names[i], 'rb') as f:\n",
    "        # with open(\"./data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, adj = tuple(objects)\n",
    "    # print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    labels = np.vstack((ally, ty))\n",
    "    # print(len(labels))\n",
    "\n",
    "    train_idx_orig = parse_index_file('cleaned_data/' + dataset_str + '/graph/' + dataset_str + '.train.index')\n",
    "    train_size = len(train_idx_orig)\n",
    "\n",
    "    val_size = train_size - x.shape[0]\n",
    "    test_size = tx.shape[0]\n",
    "\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + val_size)\n",
    "    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    \n",
    "    return sparse_to_tuple(features)\n",
    "\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    \n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    \n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "\n",
    "\n",
    "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i]\n",
    "                      for i in range(len(support))})\n",
    "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def loadWord2Vec(filename):\n",
    "    \"\"\"Read Word Vectors\"\"\"\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    word_vector_map = {}\n",
    "    file = open(filename, 'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        if(len(row) > 2):\n",
    "            vocab.append(row[0])\n",
    "            vector = row[1:]\n",
    "            length = len(vector)\n",
    "            for i in range(length):\n",
    "                vector[i] = float(vector[i])\n",
    "            embd.append(vector)\n",
    "            word_vector_map[row[0]] = vector\n",
    "    \n",
    "    print('Loaded Word Vectors!')\n",
    "    file.close()\n",
    "    return vocab, embd, word_vector_map\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def uniform(shape, scale=0.05, name=None):\n",
    "    \"\"\"Uniform init.\"\"\"\n",
    "    initial = tf.random.uniform(shape, minval=-scale, maxval=scale, dtype=tf.float64)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def glorot(shape, name=None):\n",
    "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random.uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float64)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float64)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def ones(shape, name=None):\n",
    "    \"\"\"All ones.\"\"\"\n",
    "    initial = tf.ones(shape, dtype=tf.float64)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "def sparse_dropout(x, rate, noise_shape):\n",
    "    \"\"\"\n",
    "    Dropout for sparse tensors.\n",
    "    \"\"\"\n",
    "    random_tensor = rate\n",
    "    random_tensor += tf.random.uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse.retain(x, dropout_mask)\n",
    "    return pre_out * (1./(rate))\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"\n",
    "    Wrapper for tf.matmul (sparse vs dense).\n",
    "    \"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse.sparse_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da04a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(dataset)\n",
    "\n",
    "features = sp.identity(features.shape[0])  # featureless\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3fef8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "support = [preprocess_adj(adj)]\n",
    "\n",
    "t_features = tf.SparseTensor(*features)\n",
    "t_y_train = tf.convert_to_tensor(y_train)\n",
    "t_y_val = tf.convert_to_tensor(y_val)\n",
    "t_y_test = tf.convert_to_tensor(y_test)\n",
    "tm_train_mask = tf.convert_to_tensor(train_mask)\n",
    "\n",
    "tm_val_mask = tf.convert_to_tensor(val_mask)\n",
    "tm_test_mask = tf.convert_to_tensor(test_mask)\n",
    "\n",
    "t_support = []\n",
    "for i in range(len(support)):\n",
    "    t_support.append(tf.cast(tf.SparseTensor(*support[i]), dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "786d12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(layers.Layer):\n",
    "    \"\"\"\n",
    "    Graph convolution layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero,\n",
    "                 dropout=0.,\n",
    "                 is_sparse_inputs=False,\n",
    "                 activation=tf.nn.relu,\n",
    "                 bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.is_sparse_inputs = is_sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.num_features_nonzero = num_features_nonzero\n",
    "        self.embedding = None\n",
    "\n",
    "        self.weights_ = []\n",
    "        for i in range(1):\n",
    "            w = self.add_variable('weight' + str(i), [input_dim, output_dim])\n",
    "            self.weights_.append(w)\n",
    "        if self.bias:\n",
    "            self.bias = self.add_variable('bias', [output_dim])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x, support_ = inputs\n",
    "\n",
    "        # dropout\n",
    "        if training is not False and self.is_sparse_inputs:\n",
    "            x = sparse_dropout(x, self.dropout, self.num_features_nonzero)\n",
    "        elif training is not False:\n",
    "            x = tf.nn.dropout(x, self.dropout)\n",
    "\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(len(support_)):\n",
    "            if not self.featureless: # if it has features x\n",
    "                pre_sup = dot(x, self.weights_[i], sparse=self.is_sparse_inputs)\n",
    "            else:\n",
    "                pre_sup = self.weights_[i]\n",
    "\n",
    "            support = dot(support_[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.bias\n",
    "        \n",
    "        self.embedding = output # for visualization\n",
    "        return self.activation(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dbc34396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(keras.Model):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_dim = input_dim # 1433\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        print('input  dim: ', input_dim)\n",
    "        print('output dim: ', output_dim)\n",
    "\n",
    "        self.layers_ = []\n",
    "        self.layers_.append(GraphConvolution(input_dim=self.input_dim, # 1433\n",
    "                                            output_dim=hidden1, # 16\n",
    "                                            num_features_nonzero=num_features_nonzero,\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            dropout=dropout,\n",
    "                                            is_sparse_inputs=True))\n",
    "\n",
    "\n",
    "        self.layers_.append(GraphConvolution(input_dim=hidden1, # 16\n",
    "                                            output_dim=self.output_dim, # 7\n",
    "                                            num_features_nonzero=num_features_nonzero,\n",
    "                                            activation=lambda x: x,\n",
    "                                            dropout=dropout))\n",
    "\n",
    "\n",
    "            \n",
    "    def call(self, inputs, training=None):\n",
    "        x, label, mask, support = inputs\n",
    "\n",
    "        outputs = [x]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden = layer((outputs[-1], support), training)\n",
    "            outputs.append(hidden)\n",
    "        output = outputs[-1]\n",
    "\n",
    "        # # Weight decay loss\n",
    "        loss = tf.zeros([])\n",
    "        for var in self.layers_[0].trainable_variables:\n",
    "            loss +=weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        loss += masked_softmax_cross_entropy(output, label, mask)\n",
    "\n",
    "        acc = masked_accuracy(output, label, mask)\n",
    "\n",
    "        return tf.argmax(output, 1), loss, acc\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a852c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  dim:  15362\n",
      "output dim:  8\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = GCN(input_dim=features[2][1], output_dim=y_train.shape[1], num_features_nonzero=features[1].shape)\n",
    "\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "cost_val = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9b9d920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 2.07943 train_acc= 0.08446 val_loss= 2.06114 val_acc= 0.65328 time= 5.23540\n",
      "Epoch: 0002 train_loss= 2.06178 train_acc= 0.65445 val_loss= 2.02133 val_acc= 0.70073 time= 5.06043\n",
      "Epoch: 0003 train_loss= 2.02285 train_acc= 0.66377 val_loss= 1.95837 val_acc= 0.72080 time= 4.92645\n",
      "Epoch: 0004 train_loss= 1.96015 train_acc= 0.70164 val_loss= 1.87269 val_acc= 0.73723 time= 5.21167\n",
      "Epoch: 0005 train_loss= 1.87801 train_acc= 0.70144 val_loss= 1.76673 val_acc= 0.74453 time= 4.96838\n",
      "Epoch: 0006 train_loss= 1.77486 train_acc= 0.71926 val_loss= 1.64652 val_acc= 0.74453 time= 5.27725\n",
      "Epoch: 0007 train_loss= 1.66752 train_acc= 0.71562 val_loss= 1.52308 val_acc= 0.73905 time= 5.01324\n",
      "Epoch: 0008 train_loss= 1.54614 train_acc= 0.70630 val_loss= 1.40879 val_acc= 0.73723 time= 5.09187\n",
      "Epoch: 0009 train_loss= 1.43603 train_acc= 0.71582 val_loss= 1.31238 val_acc= 0.72080 time= 5.02987\n",
      "Epoch: 0010 train_loss= 1.35458 train_acc= 0.69354 val_loss= 1.23460 val_acc= 0.68066 time= 5.32537\n",
      "Epoch: 0011 train_loss= 1.28046 train_acc= 0.66133 val_loss= 1.17108 val_acc= 0.63321 time= 5.11796\n",
      "Epoch: 0012 train_loss= 1.24182 train_acc= 0.62994 val_loss= 1.11601 val_acc= 0.60219 time= 5.07207\n",
      "Epoch: 0013 train_loss= 1.19031 train_acc= 0.60199 val_loss= 1.06483 val_acc= 0.59672 time= 4.82227\n",
      "Epoch: 0014 train_loss= 1.13350 train_acc= 0.59510 val_loss= 1.01407 val_acc= 0.62409 time= 4.87930\n",
      "Epoch: 0015 train_loss= 1.08126 train_acc= 0.60827 val_loss= 0.96231 val_acc= 0.66606 time= 4.81201\n",
      "Epoch: 0016 train_loss= 1.02841 train_acc= 0.65343 val_loss= 0.90960 val_acc= 0.73175 time= 5.05134\n",
      "Epoch: 0017 train_loss= 0.96998 train_acc= 0.70448 val_loss= 0.85824 val_acc= 0.76825 time= 4.68338\n",
      "Epoch: 0018 train_loss= 0.91845 train_acc= 0.75269 val_loss= 0.81051 val_acc= 0.78467 time= 4.91076\n",
      "Epoch: 0019 train_loss= 0.86313 train_acc= 0.77335 val_loss= 0.76844 val_acc= 0.78832 time= 4.98242\n",
      "Epoch: 0020 train_loss= 0.81652 train_acc= 0.77922 val_loss= 0.73267 val_acc= 0.78467 time= 4.76085\n",
      "Epoch: 0021 train_loss= 0.77161 train_acc= 0.77395 val_loss= 0.70264 val_acc= 0.78285 time= 4.59209\n",
      "Epoch: 0022 train_loss= 0.73962 train_acc= 0.77598 val_loss= 0.67703 val_acc= 0.77372 time= 4.69183\n",
      "Epoch: 0023 train_loss= 0.71071 train_acc= 0.77699 val_loss= 0.65414 val_acc= 0.77920 time= 4.78791\n",
      "Epoch: 0024 train_loss= 0.68804 train_acc= 0.77881 val_loss= 0.63264 val_acc= 0.79197 time= 4.75360\n",
      "Epoch: 0025 train_loss= 0.65781 train_acc= 0.79846 val_loss= 0.61162 val_acc= 0.82299 time= 4.71247\n",
      "Epoch: 0026 train_loss= 0.63978 train_acc= 0.81851 val_loss= 0.59056 val_acc= 0.83942 time= 4.95836\n",
      "Epoch: 0027 train_loss= 0.61796 train_acc= 0.82317 val_loss= 0.56921 val_acc= 0.85036 time= 4.80074\n",
      "Epoch: 0028 train_loss= 0.59590 train_acc= 0.84788 val_loss= 0.54773 val_acc= 0.86314 time= 4.77014\n",
      "Epoch: 0029 train_loss= 0.57155 train_acc= 0.85457 val_loss= 0.52633 val_acc= 0.87591 time= 5.08521\n",
      "Epoch: 0030 train_loss= 0.54147 train_acc= 0.86692 val_loss= 0.50530 val_acc= 0.87956 time= 5.23217\n",
      "Epoch: 0031 train_loss= 0.53235 train_acc= 0.87057 val_loss= 0.48472 val_acc= 0.88139 time= 4.85200\n",
      "Epoch: 0032 train_loss= 0.50166 train_acc= 0.87442 val_loss= 0.46524 val_acc= 0.88686 time= 4.72337\n",
      "Epoch: 0033 train_loss= 0.48879 train_acc= 0.88475 val_loss= 0.44684 val_acc= 0.88869 time= 5.03574\n",
      "Epoch: 0034 train_loss= 0.46490 train_acc= 0.88596 val_loss= 0.42962 val_acc= 0.89416 time= 5.29926\n",
      "Epoch: 0035 train_loss= 0.45299 train_acc= 0.88779 val_loss= 0.41344 val_acc= 0.89599 time= 4.94764\n",
      "Epoch: 0036 train_loss= 0.43674 train_acc= 0.89103 val_loss= 0.39813 val_acc= 0.89781 time= 5.00455\n",
      "Epoch: 0037 train_loss= 0.42142 train_acc= 0.89265 val_loss= 0.38358 val_acc= 0.89781 time= 4.73850\n",
      "Epoch: 0038 train_loss= 0.40119 train_acc= 0.89407 val_loss= 0.36975 val_acc= 0.90328 time= 4.91048\n",
      "Epoch: 0039 train_loss= 0.38663 train_acc= 0.89832 val_loss= 0.35659 val_acc= 0.90693 time= 5.20245\n",
      "Epoch: 0040 train_loss= 0.37432 train_acc= 0.90136 val_loss= 0.34408 val_acc= 0.91058 time= 4.79277\n",
      "Epoch: 0041 train_loss= 0.35788 train_acc= 0.90683 val_loss= 0.33230 val_acc= 0.91606 time= 4.86372\n",
      "Epoch: 0042 train_loss= 0.34888 train_acc= 0.90500 val_loss= 0.32119 val_acc= 0.92153 time= 4.59822\n",
      "Epoch: 0043 train_loss= 0.34107 train_acc= 0.91270 val_loss= 0.31070 val_acc= 0.92336 time= 4.77880\n",
      "Epoch: 0044 train_loss= 0.31426 train_acc= 0.91635 val_loss= 0.30095 val_acc= 0.92336 time= 4.91792\n",
      "Epoch: 0045 train_loss= 0.30383 train_acc= 0.92141 val_loss= 0.29189 val_acc= 0.92518 time= 4.73175\n",
      "Epoch: 0046 train_loss= 0.30016 train_acc= 0.92627 val_loss= 0.28341 val_acc= 0.93248 time= 5.01768\n",
      "Epoch: 0047 train_loss= 0.28408 train_acc= 0.92809 val_loss= 0.27552 val_acc= 0.93248 time= 5.20447\n",
      "Epoch: 0048 train_loss= 0.26875 train_acc= 0.93539 val_loss= 0.26791 val_acc= 0.93613 time= 5.03567\n",
      "Epoch: 0049 train_loss= 0.26574 train_acc= 0.94025 val_loss= 0.26046 val_acc= 0.94161 time= 5.38472\n",
      "Epoch: 0050 train_loss= 0.24755 train_acc= 0.94450 val_loss= 0.25291 val_acc= 0.94161 time= 5.31213\n",
      "Epoch: 0051 train_loss= 0.23795 train_acc= 0.94389 val_loss= 0.24530 val_acc= 0.94526 time= 4.91017\n",
      "Epoch: 0052 train_loss= 0.23741 train_acc= 0.94491 val_loss= 0.23775 val_acc= 0.94890 time= 5.25989\n",
      "Epoch: 0053 train_loss= 0.22281 train_acc= 0.94956 val_loss= 0.23063 val_acc= 0.95073 time= 5.00950\n",
      "Epoch: 0054 train_loss= 0.21531 train_acc= 0.95199 val_loss= 0.22365 val_acc= 0.95073 time= 4.88449\n",
      "Epoch: 0055 train_loss= 0.20410 train_acc= 0.95483 val_loss= 0.21672 val_acc= 0.94890 time= 5.73890\n",
      "Epoch: 0056 train_loss= 0.19882 train_acc= 0.95301 val_loss= 0.21017 val_acc= 0.95073 time= 4.94732\n",
      "Epoch: 0057 train_loss= 0.19398 train_acc= 0.95645 val_loss= 0.20399 val_acc= 0.95073 time= 5.13096\n",
      "Epoch: 0058 train_loss= 0.18716 train_acc= 0.95564 val_loss= 0.19816 val_acc= 0.94890 time= 5.16138\n",
      "Epoch: 0059 train_loss= 0.17594 train_acc= 0.95726 val_loss= 0.19276 val_acc= 0.94890 time= 4.85318\n",
      "Epoch: 0060 train_loss= 0.17108 train_acc= 0.95746 val_loss= 0.18769 val_acc= 0.95073 time= 5.69486\n",
      "Epoch: 0061 train_loss= 0.16226 train_acc= 0.96050 val_loss= 0.18302 val_acc= 0.95073 time= 5.32442\n",
      "Epoch: 0062 train_loss= 0.15841 train_acc= 0.96415 val_loss= 0.17883 val_acc= 0.95073 time= 5.01483\n",
      "Epoch: 0063 train_loss= 0.15960 train_acc= 0.95868 val_loss= 0.17513 val_acc= 0.95255 time= 5.27526\n",
      "Epoch: 0064 train_loss= 0.14767 train_acc= 0.96253 val_loss= 0.17163 val_acc= 0.95255 time= 4.82992\n",
      "Epoch: 0065 train_loss= 0.14193 train_acc= 0.96172 val_loss= 0.16814 val_acc= 0.95620 time= 4.92199\n",
      "Epoch: 0066 train_loss= 0.13726 train_acc= 0.96536 val_loss= 0.16512 val_acc= 0.95803 time= 5.36170\n",
      "Epoch: 0067 train_loss= 0.13462 train_acc= 0.96759 val_loss= 0.16239 val_acc= 0.95985 time= 5.33182\n",
      "Epoch: 0068 train_loss= 0.12687 train_acc= 0.96779 val_loss= 0.15994 val_acc= 0.95803 time= 5.41947\n",
      "Epoch: 0069 train_loss= 0.11949 train_acc= 0.97083 val_loss= 0.15745 val_acc= 0.95438 time= 5.15118\n",
      "Epoch: 0070 train_loss= 0.12180 train_acc= 0.97002 val_loss= 0.15460 val_acc= 0.95438 time= 5.38407\n",
      "Epoch: 0071 train_loss= 0.11276 train_acc= 0.97265 val_loss= 0.15136 val_acc= 0.95985 time= 5.20521\n",
      "Epoch: 0072 train_loss= 0.11350 train_acc= 0.97225 val_loss= 0.14792 val_acc= 0.95985 time= 5.00965\n",
      "Epoch: 0073 train_loss= 0.10748 train_acc= 0.97184 val_loss= 0.14502 val_acc= 0.95803 time= 4.66852\n",
      "Epoch: 0074 train_loss= 0.10519 train_acc= 0.97650 val_loss= 0.14191 val_acc= 0.96168 time= 5.13744\n",
      "Epoch: 0075 train_loss= 0.09943 train_acc= 0.97488 val_loss= 0.13869 val_acc= 0.96350 time= 5.18684\n",
      "Epoch: 0076 train_loss= 0.09886 train_acc= 0.97671 val_loss= 0.13535 val_acc= 0.96350 time= 5.22248\n",
      "Epoch: 0077 train_loss= 0.09231 train_acc= 0.98116 val_loss= 0.13215 val_acc= 0.96350 time= 5.06475\n",
      "Epoch: 0078 train_loss= 0.09395 train_acc= 0.97853 val_loss= 0.12938 val_acc= 0.96350 time= 4.85037\n",
      "Epoch: 0079 train_loss= 0.09165 train_acc= 0.97650 val_loss= 0.12708 val_acc= 0.96533 time= 5.15057\n",
      "Epoch: 0080 train_loss= 0.09064 train_acc= 0.97873 val_loss= 0.12529 val_acc= 0.96715 time= 4.68162\n",
      "Epoch: 0081 train_loss= 0.08635 train_acc= 0.97934 val_loss= 0.12393 val_acc= 0.96715 time= 5.06741\n",
      "Epoch: 0082 train_loss= 0.08251 train_acc= 0.97772 val_loss= 0.12300 val_acc= 0.96898 time= 4.89665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0083 train_loss= 0.08204 train_acc= 0.97914 val_loss= 0.12249 val_acc= 0.96898 time= 5.29085\n",
      "Epoch: 0084 train_loss= 0.08427 train_acc= 0.97792 val_loss= 0.12214 val_acc= 0.96898 time= 5.04601\n",
      "Epoch: 0085 train_loss= 0.08151 train_acc= 0.98096 val_loss= 0.12120 val_acc= 0.97080 time= 5.20162\n",
      "Epoch: 0086 train_loss= 0.07606 train_acc= 0.98096 val_loss= 0.11979 val_acc= 0.97080 time= 4.71493\n",
      "Epoch: 0087 train_loss= 0.07873 train_acc= 0.97995 val_loss= 0.11808 val_acc= 0.97080 time= 4.83650\n",
      "Epoch: 0088 train_loss= 0.07633 train_acc= 0.98319 val_loss= 0.11657 val_acc= 0.97263 time= 4.91131\n",
      "Epoch: 0089 train_loss= 0.07479 train_acc= 0.98076 val_loss= 0.11627 val_acc= 0.97263 time= 4.96424\n",
      "Epoch: 0090 train_loss= 0.07150 train_acc= 0.98197 val_loss= 0.11465 val_acc= 0.97080 time= 5.24305\n",
      "Epoch: 0091 train_loss= 0.06709 train_acc= 0.98298 val_loss= 0.11282 val_acc= 0.96898 time= 5.26377\n",
      "Epoch: 0092 train_loss= 0.06470 train_acc= 0.98440 val_loss= 0.11152 val_acc= 0.96533 time= 4.84557\n",
      "Epoch: 0093 train_loss= 0.07015 train_acc= 0.98096 val_loss= 0.10996 val_acc= 0.96715 time= 4.85295\n",
      "Epoch: 0094 train_loss= 0.06491 train_acc= 0.98744 val_loss= 0.10886 val_acc= 0.96715 time= 5.11231\n",
      "Epoch: 0095 train_loss= 0.06224 train_acc= 0.98623 val_loss= 0.10845 val_acc= 0.96715 time= 4.84220\n",
      "Epoch: 0096 train_loss= 0.06401 train_acc= 0.98197 val_loss= 0.10913 val_acc= 0.96715 time= 5.22907\n",
      "Epoch: 0097 train_loss= 0.06121 train_acc= 0.98805 val_loss= 0.11042 val_acc= 0.97080 time= 5.55093\n",
      "Epoch: 0098 train_loss= 0.06067 train_acc= 0.98461 val_loss= 0.11058 val_acc= 0.97080 time= 5.63660\n",
      "Epoch: 0099 train_loss= 0.05663 train_acc= 0.98704 val_loss= 0.11035 val_acc= 0.97080 time= 5.39911\n",
      "Epoch: 0100 train_loss= 0.05878 train_acc= 0.98643 val_loss= 0.10945 val_acc= 0.96898 time= 7.24023\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    t = time.time()\n",
    "    with tf.GradientTape() as tape:\n",
    "        _, loss, acc = model((t_features, t_y_train, tm_train_mask, t_support))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    _, val_loss, val_acc = model((t_features, t_y_val, tm_val_mask, t_support), training=False)\n",
    "    cost_val.append(val_loss)\n",
    "    \n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(loss),\n",
    "          \"train_acc=\", \"{:.5f}\".format(acc), \"val_loss=\", \"{:.5f}\".format(val_loss),\n",
    "          \"val_acc=\", \"{:.5f}\".format(val_acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "    \n",
    "    if epoch > early_stopping and cost_val[-1] > np.mean(cost_val[-(early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee6dfeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: cost= 0.11332 accuracy= 0.97259 time= 1.04790\n",
      "Average Test Precision, Recall and F1-Score...\n",
      "(0.9725902238465053, 0.9725902238465053, 0.9725902238465053, None)\n"
     ]
    }
   ],
   "source": [
    "def evaluate(features, y, mask, support):\n",
    "    t = time.time()\n",
    "    \n",
    "    pred, test_loss, test_acc = model((features, y, mask, support), training=False)\n",
    "    \n",
    "    \n",
    "    return test_loss, test_acc, pred, np.argmax(y, axis=1), time.time() - t\n",
    "\n",
    "\n",
    "test_cost, test_acc, pred, labels, test_duration = evaluate(t_features, t_y_test, tm_test_mask, t_support)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost), \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "\n",
    "\n",
    "test_pred = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(len(test_mask)):\n",
    "    if test_mask[i]:\n",
    "        test_pred.append(pred[i])\n",
    "        test_labels.append(labels[i])\n",
    "\n",
    "print(\"Average Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7403bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
