{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "Dm8q2WLKDKH0",
   "metadata": {
    "id": "Dm8q2WLKDKH0"
   },
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import scipy.sparse as sp\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import sys\n",
    "sys.path.append('./')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "X8Zx-cuvDKH3",
   "metadata": {
    "id": "X8Zx-cuvDKH3"
   },
   "outputs": [],
   "source": [
    "#Initializing variables\n",
    "dataset = 'data'\n",
    "\n",
    "word_embeds_dim = 300\n",
    "word2vec_map = {}\n",
    "\n",
    "#shulffing\n",
    "doc_names = []\n",
    "doc_train = []\n",
    "doc_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "HttLVAZYDKH3",
   "metadata": {
    "id": "HttLVAZYDKH3"
   },
   "outputs": [],
   "source": [
    "#Splitting data into train and test data files\n",
    "with open('./cleaned_data2/' + dataset + '/' + dataset + '.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    lines = fp.readlines()\n",
    "    for l in lines:\n",
    "        doc_names.append(l.strip())\n",
    "        col = l.split(\",\")\n",
    "        if col[3].find('test') != -1:\n",
    "            doc_test.append(l.strip())\n",
    "        elif col[3].find('train') != -1:\n",
    "            doc_train.append(l.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "rt_EHb8UDKH4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1632573834934,
     "user": {
      "displayName": "Palak techies",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04966026704557831019"
     },
     "user_tz": -330
    },
    "id": "rt_EHb8UDKH4",
    "outputId": "937086c1-2dcb-43ef-e69f-c1635d75893f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff0,Safe and effective ,1,train'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "P6_1ICcsDKH4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1632573834936,
     "user": {
      "displayName": "Palak techies",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04966026704557831019"
     },
     "user_tz": -330
    },
    "id": "P6_1ICcsDKH4",
    "outputId": "f2f1bda0-6d61-4d2d-e30c-4927351c5509"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff0,Safe and effective ,1,train'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bM_M6tabDKH5",
   "metadata": {
    "id": "bM_M6tabDKH5"
   },
   "outputs": [],
   "source": [
    "doc_cont_list = []\n",
    "with open('./cleaned_data2/' + dataset + '/' + dataset + '_clean.txt', 'r') as fp:\n",
    "    lines = fp.readlines()\n",
    "    for l in lines:\n",
    "        doc_cont_list.append(l.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "zkBugtULDKH5",
   "metadata": {
    "id": "zkBugtULDKH5"
   },
   "outputs": [],
   "source": [
    "#shuffling train data\n",
    "train_ids = []\n",
    "for train_name in doc_train:\n",
    "    id = doc_names.index(train_name)\n",
    "    train_ids.append(id)\n",
    "random.shuffle(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "qAijD5uVDKH6",
   "metadata": {
    "id": "qAijD5uVDKH6"
   },
   "outputs": [],
   "source": [
    "#getting all train ids in separate file\n",
    "train_ids_st = '\\n'.join(str(i) for i in train_ids)\n",
    "with open('./cleaned_data2/' + dataset + '/graph/' + dataset + '.train.index', 'w') as fp:\n",
    "    fp.write(train_ids_st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "OoIQdgSwDKH6",
   "metadata": {
    "id": "OoIQdgSwDKH6"
   },
   "outputs": [],
   "source": [
    "#shuffling test data\n",
    "test_ids = []\n",
    "for test_name in doc_test:\n",
    "    id = doc_names.index(test_name)\n",
    "    test_ids.append(id)\n",
    "\n",
    "random.shuffle(test_ids)\n",
    "\n",
    "#getting all train ids in separate file\n",
    "test_ids_str = '\\n'.join(str(i) for i in test_ids)\n",
    "with open('./cleaned_data2/' + dataset + '/graph/' + dataset + '.test.index', 'w') as fp:\n",
    "    fp.write(test_ids_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "LHdhJiOvDKH7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1632573842082,
     "user": {
      "displayName": "Palak techies",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04966026704557831019"
     },
     "user_tz": -330
    },
    "id": "LHdhJiOvDKH7",
    "outputId": "be04caf8-e87a-4a74-80e6-e44b9255f3c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size 23898\n",
      "adding up...\n"
     ]
    }
   ],
   "source": [
    "#merging train and test ids\n",
    "total_ids = train_ids + test_ids\n",
    "print(\"dataset size {}\".format(len(total_ids)))\n",
    "\n",
    "print(\"adding up...\")\n",
    "shuffled_doc_names = []\n",
    "shuffled_doc_words = []\n",
    "for id_ in total_ids:\n",
    "    shuffled_doc_names.append(doc_names[int(id_)])\n",
    "    shuffled_doc_words.append(doc_cont_list[int(id_)])\n",
    "shuffled_doc_names_str = '\\n'.join(shuffled_doc_names)\n",
    "shuffled_doc_words_str = '\\n'.join(shuffled_doc_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "jKRF8gNUDKH7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1632573842084,
     "user": {
      "displayName": "Palak techies",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04966026704557831019"
     },
     "user_tz": -330
    },
    "id": "jKRF8gNUDKH7",
    "outputId": "72cc8ed3-acc8-4dd2-a150-eb4fb2d3d71f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grand central oyster bar opening 100 capacity monday chef sandy ingber joins daily briefing promote vaccination'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_doc_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "FiHqovH3DKH7",
   "metadata": {
    "id": "FiHqovH3DKH7"
   },
   "outputs": [],
   "source": [
    "with open('./cleaned_data2/' + dataset + '/' + dataset + '_shuffle.txt', 'w', encoding=\"utf8\") as fp:\n",
    "    fp.write(shuffled_doc_names_str)\n",
    "\n",
    "with open('./cleaned_data2/' + dataset + '/corpus/' + dataset + '_shuffle.txt', 'w', encoding=\"utf8\") as fp:\n",
    "    fp.write(shuffled_doc_words_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "n0RD-ubWDKH8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 909,
     "status": "ok",
     "timestamp": 1632573842971,
     "user": {
      "displayName": "Palak techies",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04966026704557831019"
     },
     "user_tz": -330
    },
    "id": "n0RD-ubWDKH8",
    "outputId": "0dfe117d-521c-44e7-c081-f43ab4798a67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab...\n"
     ]
    }
   ],
   "source": [
    "# building  vocab\n",
    "\n",
    "print(\"building vocab...\")\n",
    "word_freq = {}\n",
    "wordset = set()\n",
    "for doc_words in shuffled_doc_words:\n",
    "    words = doc_words.split()\n",
    "    for w in words:\n",
    "        wordset.add(w)\n",
    "        if w in word_freq:\n",
    "            word_freq[w] += 1\n",
    "        else:\n",
    "            word_freq[w] = 1\n",
    "\n",
    "vocab = list(wordset)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "r5yWSnmgDKH8",
   "metadata": {
    "id": "r5yWSnmgDKH8"
   },
   "outputs": [],
   "source": [
    "#document having all words\n",
    "word_doc_ls = {}\n",
    "\n",
    "for i in range(len(shuffled_doc_words)):\n",
    "    doc_words = shuffled_doc_words[i]\n",
    "    words = doc_words.split()\n",
    "    appearance = set()\n",
    "    for w in words:\n",
    "        if w in appearance:\n",
    "            continue\n",
    "        if w in word_doc_ls:\n",
    "            doc_list = word_doc_ls[w]\n",
    "            doc_list.append(i)\n",
    "            word_doc_ls[w] = doc_list\n",
    "        else:\n",
    "            word_doc_ls[w] = [i]\n",
    "        appearance.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "sqh8zwmDDKH8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1632573842977,
     "user": {
      "displayName": "Palak techies",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04966026704557831019"
     },
     "user_tz": -330
    },
    "id": "sqh8zwmDDKH8",
    "outputId": "4bb2af68-7979-494f-df11-0276967765bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cross', 'mean', 'tough', 'vaccine', 'canada', 'likely', 'next', 'play', 'border', 'impossible', 'without', 'gonna', 'night'}\n"
     ]
    }
   ],
   "source": [
    "print(appearance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ji4-QOUGDKH9",
   "metadata": {
    "id": "ji4-QOUGDKH9"
   },
   "outputs": [],
   "source": [
    "#vocab with frequency\n",
    "worddoc_frequency = {}\n",
    "for w, doc_list in word_doc_ls.items():\n",
    "    worddoc_frequency[w] = len(doc_list)\n",
    "\n",
    "word_ids = {}\n",
    "for i in range(vocab_size):\n",
    "    word_ids[vocab[i]] = i\n",
    "\n",
    "vocab_str = '\\n'.join(vocab)\n",
    "\n",
    "with open('./cleaned_data2/' + dataset + '/corpus/' + dataset + '_vocab.txt', 'w') as fp:\n",
    "    fp.write(vocab_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ktNOVg_BDKH9",
   "metadata": {
    "id": "ktNOVg_BDKH9"
   },
   "outputs": [],
   "source": [
    "# labels list\n",
    "labels_set = set()\n",
    "for doc_met in shuffled_doc_names:\n",
    "    col = doc_met.split(',')\n",
    "    labels_set.add(col[2])\n",
    "labels_list = list(labels_set)\n",
    "\n",
    "labels_list_str = '\\n'.join(labels_list)\n",
    "with open('./cleaned_data2/' + dataset + '/corpus/' + dataset + '_labels.txt', 'w') as fp:\n",
    "    fp.write(labels_list_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ZUXtDU78DKH9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1632573842984,
     "user": {
      "displayName": "Palak techies",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04966026704557831019"
     },
     "user_tz": -330
    },
    "id": "ZUXtDU78DKH9",
    "outputId": "a08c6966-579e-4a76-d6f7-698c5f694bb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0\\n1\\n2'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4g9dVJoDKH9",
   "metadata": {
    "id": "f4g9dVJoDKH9"
   },
   "outputs": [],
   "source": [
    "#train\n",
    "train_size = len(train_ids)\n",
    "valid_size = int(0.1 * train_size)\n",
    "actual_train_size = train_size - valid_size \n",
    "\n",
    "\n",
    "actual_train_doc_names = shuffled_doc_names[:actual_train_size]\n",
    "actual_train_doc_names_str = '\\n'.join(actual_train_doc_names)\n",
    "\n",
    "with open('./cleaned_data2/' + dataset + '/graph/' + dataset + '.real_train.name', 'w',encoding='utf-8') as fp:\n",
    "    fp.write(actual_train_doc_names_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "iDBX8ZXJDKH-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4937,
     "status": "ok",
     "timestamp": 1632573848315,
     "user": {
      "displayName": "Palak techies",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04966026704557831019"
     },
     "user_tz": -330
    },
    "id": "iDBX8ZXJDKH-",
    "outputId": "754433f6-70b2-4716-ac52-5e2584b93994"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\env\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "#separating features data\n",
    "rows_x = []\n",
    "cols_x = []\n",
    "data_x = []\n",
    "for i in range(actual_train_size):\n",
    "    doc_vector = np.array([0.0 for k in range(word_embeds_dim)])\n",
    "    doc_words = shuffled_doc_words[i]\n",
    "    words = doc_words.split()\n",
    "    doc_length = len(words)\n",
    "    for w in words:\n",
    "        if w in word2vec_map:\n",
    "            word_vector = word2vec_map[w]\n",
    "            doc_vector = doc_vector + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeds_dim):\n",
    "        rows_x.append(i)\n",
    "        cols_x.append(j)\n",
    "        data_x.append(doc_vector[j] / doc_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "Rqb-bDGaDKH-",
   "metadata": {
    "id": "Rqb-bDGaDKH-"
   },
   "outputs": [],
   "source": [
    "#separating data into x and y for train data\n",
    "X = sp.csr_matrix((data_x, (rows_x, cols_x)), shape=(actual_train_size, word_embeds_dim))\n",
    "y = []\n",
    "for i in range(actual_train_size):\n",
    "    doc_met = shuffled_doc_names[i]\n",
    "    cols = doc_met.split(',')\n",
    "    label = cols[2]\n",
    "    one_hot = [0 for lab in range(len(labels_list))]\n",
    "    label_ind = labels_list.index(label)\n",
    "    one_hot[label_ind] = 1\n",
    "    y.append(one_hot)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bxM26RUQDKH-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1699,
     "status": "ok",
     "timestamp": 1632573852271,
     "user": {
      "displayName": "Palak techies",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04966026704557831019"
     },
     "user_tz": -330
    },
    "id": "bxM26RUQDKH-",
    "outputId": "ce82163f-4928-45e9-dedf-1c81f88acbfd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\env\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# feature vectors of test document, no initial features\n",
    "test_size = len(test_ids)\n",
    "\n",
    "rows_tx = []\n",
    "cols_tx = []\n",
    "data_tx = []\n",
    "for i in range(test_size):\n",
    "    doc_vector = np.array([0.0 for k in range(word_embeds_dim)])\n",
    "    doc_words = shuffled_doc_words[i + train_size]\n",
    "    words = doc_words.split()\n",
    "    doc_length = len(words)\n",
    "    for w in words:\n",
    "        if w in word2vec_map:\n",
    "            word_vector = word2vec_map[w]\n",
    "            doc_vector = doc_vector + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeds_dim):\n",
    "        rows_tx.append(i)\n",
    "        cols_tx.append(j)\n",
    "       \n",
    "        data_tx.append(doc_vector[j] / doc_length)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "hIof55F0DKH-",
   "metadata": {
    "id": "hIof55F0DKH-"
   },
   "outputs": [],
   "source": [
    "#separating data into x and y for test data\n",
    "tx = sp.csr_matrix((data_tx, (rows_tx, cols_tx)),shape=(test_size, word_embeds_dim))\n",
    "ty = []\n",
    "for i in range(test_size):\n",
    "    doc_met = shuffled_doc_names[i + train_size]\n",
    "    cols = doc_met.split(',')\n",
    "    label = cols[2]\n",
    "    one_hot = [0 for lab in range(len(labels_list))]\n",
    "    label_ind = labels_list.index(label)\n",
    "    one_hot[label_ind] = 1\n",
    "    ty.append(one_hot)\n",
    "ty = np.array(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "GOSEMOXMDKH_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10530,
     "status": "ok",
     "timestamp": 1632573863394,
     "user": {
      "displayName": "Palak techies",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04966026704557831019"
     },
     "user_tz": -330
    },
    "id": "GOSEMOXMDKH_",
    "outputId": "54a4e474-867f-4e4c-c2a5-8b5ba5a97ffb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\env\\lib\\site-packages\\ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17189, 300) (17189, 3) (4800, 300) (4800, 3) (25298, 300) (25298, 3)\n"
     ]
    }
   ],
   "source": [
    "#all x data  and all y data\n",
    "word_vecs = np.random.uniform(-0.01, 0.01,(vocab_size, word_embeds_dim))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    w = vocab[i]\n",
    "    if w in word2vec_map:\n",
    "        vector = word2vec_map[w]\n",
    "        word_vecs[i] = vector\n",
    "\n",
    "row_all_x = []\n",
    "col_all_x = []\n",
    "data_all_x = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    doc_vector = np.array([0.0 for k in range(word_embeds_dim)])\n",
    "    doc_words = shuffled_doc_words[i]\n",
    "    words = doc_words.split()\n",
    "    doc_length = len(words)\n",
    "    for w in words:\n",
    "        if w in word2vec_map:\n",
    "            word_vec = word2vec_map[w]\n",
    "            doc_vector = doc_vector + np.array(word_vec)\n",
    "\n",
    "    for j in range(word_embeds_dim):\n",
    "        row_all_x.append(int(i))\n",
    "        col_all_x.append(j)\n",
    "       \n",
    "        data_all_x.append(doc_vector[j] / doc_length) \n",
    "for i in range(vocab_size):\n",
    "    for j in range(word_embeds_dim):\n",
    "        row_all_x.append(int(i + train_size))\n",
    "        col_all_x.append(j)\n",
    "        data_all_x.append(word_vecs.item((i, j)))\n",
    "\n",
    "\n",
    "row_all_x = np.array(row_all_x)\n",
    "col_all_x = np.array(col_all_x)\n",
    "data_all_x = np.array(data_all_x)\n",
    "\n",
    "all_x = sp.csr_matrix((data_all_x, (row_all_x, col_all_x)), shape=(train_size + vocab_size, word_embeds_dim))\n",
    "\n",
    "all_y = []\n",
    "for i in range(train_size):\n",
    "    doc_met = shuffled_doc_names[i]\n",
    "    cols = doc_met.split(',')\n",
    "    label = cols[2]\n",
    "    one_hot = [0 for l in range(len(labels_list))]\n",
    "    label_ind = labels_list.index(label)\n",
    "    one_hot[label_ind] = 1\n",
    "    all_y.append(one_hot)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    one_hot = [0 for l in range(len(labels_list))]\n",
    "    all_y.append(one_hot)\n",
    "\n",
    "all_y = np.array(all_y)\n",
    "\n",
    "print(X.shape, y.shape, tx.shape, ty.shape, all_x.shape, all_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "afmPo-yXDKH_",
   "metadata": {
    "id": "afmPo-yXDKH_"
   },
   "outputs": [],
   "source": [
    "# words co-occurence with the context\n",
    "win_size = 20\n",
    "win = []\n",
    "\n",
    "for doc_words in shuffled_doc_words:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= win_size:\n",
    "        win.append(words)\n",
    "    else:\n",
    "        for j in range(length - win_size + 1):\n",
    "            wind = words[j: j + win_size]\n",
    "            win.append(wind)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6VEVvcnM6QGf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1632573863398,
     "user": {
      "displayName": "Palak techies",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04966026704557831019"
     },
     "user_tz": -330
    },
    "id": "6VEVvcnM6QGf",
    "outputId": "fb24193a-ae15-4711-f4f2-55474be356aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grand',\n",
       " 'central',\n",
       " 'oyster',\n",
       " 'bar',\n",
       " 'opening',\n",
       " '100',\n",
       " 'capacity',\n",
       " 'monday',\n",
       " 'chef',\n",
       " 'sandy',\n",
       " 'ingber',\n",
       " 'joins',\n",
       " 'daily',\n",
       " 'briefing',\n",
       " 'promote',\n",
       " 'vaccination']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "UmDqOD9PDKH_",
   "metadata": {
    "id": "UmDqOD9PDKH_"
   },
   "outputs": [],
   "source": [
    "#word window frequency\n",
    "word_win_freq = {}\n",
    "for win_ in win:\n",
    "    appearance = set()\n",
    "    for i in range(len(win_)):\n",
    "        if win_[i] in appearance:\n",
    "            continue\n",
    "        if win_[i] in word_win_freq:\n",
    "            word_win_freq[win_[i]] += 1\n",
    "        else:\n",
    "            word_win_freq[win_[i]] = 1\n",
    "        appearance.add(win_[i])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "Y3UBTFXWDKH_",
   "metadata": {
    "id": "Y3UBTFXWDKH_"
   },
   "outputs": [],
   "source": [
    "#pairs count\n",
    "word_pairs_count = {}\n",
    "for win_ in win:\n",
    "    for i in range(1, len(win_)):\n",
    "        for j in range(0, i):\n",
    "            word_i = win_[i]\n",
    "            word_i_id = word_ids[word_i]\n",
    "            word_j = win_[j]\n",
    "            word_j_id = word_ids[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pairs_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pairs_str in word_pairs_count:\n",
    "                word_pairs_count[word_pairs_str] += 1\n",
    "            else:\n",
    "                word_pairs_count[word_pairs_str] = 1\n",
    "            # two orders\n",
    "            word_pairs_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pairs_str in word_pairs_count:\n",
    "                word_pairs_count[word_pairs_str] += 1\n",
    "            else:\n",
    "                word_pairs_count[word_pairs_str] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "MvRjW6diDKH_",
   "metadata": {
    "id": "MvRjW6diDKH_"
   },
   "outputs": [],
   "source": [
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "# pmi as weights\n",
    "\n",
    "num_window = len(win)\n",
    "\n",
    "for key in word_pairs_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pairs_count[key]\n",
    "    word_freq_i = word_win_freq[vocab[i]]\n",
    "    word_freq_j = word_win_freq[vocab[j]]\n",
    "    pmi = log((1.0 * count / num_window) / (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size + i)\n",
    "    col.append(train_size + j)\n",
    "    weight.append(pmi)\n",
    "\n",
    "\n",
    "# docs-word frequency\n",
    "docs_word_freq = {}\n",
    "\n",
    "for id in range(len(shuffled_doc_words)):\n",
    "    doc_words = shuffled_doc_words[id]\n",
    "    words = doc_words.split()\n",
    "    for w in words:\n",
    "        word_id = word_ids[w]\n",
    "        doc_word_str = str(id) + ',' + str(word_id)\n",
    "        if doc_word_str in docs_word_freq:\n",
    "            docs_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            docs_word_freq[doc_word_str] = 1\n",
    "\n",
    "for i in range(len(shuffled_doc_words)):\n",
    "    doc_words = shuffled_doc_words[i]\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    for w in words:\n",
    "        if w in doc_word_set:\n",
    "            continue\n",
    "        j = word_ids[w]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = docs_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(shuffled_doc_words) /\n",
    "                  worddoc_frequency[vocab[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(w)\n",
    "\n",
    "node_size = train_size + vocab_size + test_size\n",
    "adj = sp.csr_matrix(\n",
    "    (weight, (row, col)), shape=(node_size, nod_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "yxsoNWifDKIA",
   "metadata": {
    "id": "yxsoNWifDKIA"
   },
   "outputs": [],
   "source": [
    "# dump objects\n",
    "with open('./cleaned_data2/' + dataset + '/graph/ind.' + dataset + '.x', 'wb') as fp:\n",
    "    pkl.dump(X, fp)\n",
    "\n",
    "with open('./cleaned_data2/' + dataset + '/graph/ind.' + dataset + '.y', 'wb') as fp:\n",
    "    pkl.dump(y, fp)\n",
    "\n",
    "with open('./cleaned_data2/' + dataset + '/graph/ind.' + dataset + '.tx', 'wb') as fp:\n",
    "    pkl.dump(tx, fp)\n",
    "\n",
    "with open('./cleaned_data2/' + dataset + '/graph/ind.' + dataset + '.ty', 'wb') as fp:\n",
    "    pkl.dump(ty, fp)\n",
    "\n",
    "with open('./cleaned_data2/' + dataset + '/graph/ind.' + dataset + '.allx', 'wb') as fp:\n",
    "    pkl.dump(all_x, fp)\n",
    "\n",
    "with open('./cleaned_data2/' + dataset + '/graph/ind.' + dataset + '.ally', 'wb') as fp:\n",
    "    pkl.dump(all_y, fp)\n",
    "\n",
    "with open('./cleaned_data2/' + dataset + '/graph/ind.' + dataset + '.adj', 'wb') as fp:\n",
    "    pkl.dump(adj, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-OUFGCIvDKIA",
   "metadata": {
    "id": "-OUFGCIvDKIA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "build_graph.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
